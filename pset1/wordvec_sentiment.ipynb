{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Word Vectors and Sentiment Analysis\n",
    "CS 224D Assignment 1  \n",
    "Spring 2015\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://cs224d.stanford.edu/assignment1) on the course website.*\n",
    "\n",
    "In this assignment, we will walk you through the process of implementing \n",
    "\n",
    "- A softmax function\n",
    "- A simple neural network\n",
    "- Back propagation\n",
    "- Word2vec models\n",
    "\n",
    "and training your own word vectors with stochastic gradient descent (SGD) for a sentiment analysis task. Please make sure to finish the corresponding problems in the problem set PDF when instructed by the worksheet.\n",
    "\n",
    "The purpose of this assignment is to familiarize you with basic knowledge about neural networks and machine learning, including optimization and cross-validation, and help you gain proficiency in writing efficient, vectorized code.\n",
    "\n",
    "** Please don't add or remove any code cells, as it might break our automatic grading system and affect your grade. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honor Code:** I hereby agree to abide the Stanford Honor Code and that of the Computer Science Department, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
    "\n",
    "**Signature**: kmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from cs224d.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax\n",
    "*Please answer the first first complementary problem before starting this part.*\n",
    "\n",
    "Given an input matrix of *N* rows and *d* columns, compute the softmax prediction for each row. That is, when the input is\n",
    "\n",
    "    [[1,2],\n",
    "    [3,4]]\n",
    "    \n",
    "the output of your functions should be\n",
    "\n",
    "    [[0.2689, 0.7311],\n",
    "    [0.2689, 0.7311]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.                             #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # here we take advantage of the fact that softmax is \"overparameterized\"\n",
    "    # \n",
    "    # http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "    \n",
    "    c = np.max(x, x.ndim - 1)\n",
    "    if x.ndim > 1:\n",
    "        x = x - c.reshape(len(c), 1) # so the subtraction occurs row wise\n",
    "    else:\n",
    "        x = x - c\n",
    "\n",
    "    x = np.exp(x)\n",
    "    s = np.sum(x, x.ndim - 1)\n",
    "\n",
    "    if x.ndim > 1:\n",
    "        s = s.reshape(len(s), 1) # so the normalization occurs row wise\n",
    "\n",
    "    x = x / s\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "# Verify your softmax implementation\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "print softmax(np.array([[1001,1002],[3,4]]))\n",
    "print softmax(np.array([[-1001,-1002]]))\n",
    "print softmax(np.array([[1,2], [3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network basics\n",
    "\n",
    "*Please answer the second complementary question before starting this part.*\n",
    "\n",
    "In this part, you're going to implement\n",
    "\n",
    "* A sigmoid activation function and its gradient\n",
    "* A forward propagation for a simple neural network with cross-entropy cost\n",
    "* A backward propagation algorithm to compute gradients for the parameters\n",
    "* Gradient / derivative check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x = 1 / (1+np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    f = f * (1-f)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.5         0.73105858  0.88079708]\n",
      " [ 0.5         0.26894142  0.11920292]]\n",
      "[[ 0.25        0.19661193  0.10499359]\n",
      " [ 0.25        0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "# Check your sigmoid implementation\n",
    "x = np.array([[0, 1, 2], [0, -1, -2]])\n",
    "f = sigmoid(x)\n",
    "g = sigmoid_grad(f)\n",
    "print \"=== For autograder ===\"\n",
    "print f\n",
    "print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the functions you just implemented, fill in the following functions to implement a neural network with one sigmoid hidden layer. You might find the handout and your answers to the second complementary problem helpful for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        \n",
    "        #basically we are trying to check that our gradient function produces very similar values to those\n",
    "        #that would be produced if we just re-evaluated f at a slight delta away and then subtracted\n",
    "        orig = x[ix]\n",
    "        x[ix] += h\n",
    "        random.setstate(rndstate)\n",
    "        fx_high, _ = f(x)\n",
    "\n",
    "        x[ix] = orig - h\n",
    "        random.setstate(rndstate)\n",
    "        fx_low, _ = f(x)\n",
    "        numgrad = (fx_high - fx_low)/(2*h)\n",
    "\n",
    "        x[ix] = orig\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for the gradient checker\n",
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up fake data and parameters for the neural network\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in xrange(N):\n",
    "    labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2]))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    m = data.shape[0] # number of examples\n",
    "    Z2 = data.dot(W1)\n",
    "    I2 = Z2 + b1\n",
    "    A2 = sigmoid(I2)\n",
    "\n",
    "    # print '   b1: %s' % str(b1)\n",
    "    # print 'Z2[0]: %s' % str(Z2[0:1])\n",
    "    # print 'I2[0]: %s' % str(I2[0:1])\n",
    "    # print 'A2[0]: %s' % str(A2[0:1])\n",
    "\n",
    "    Z3 = A2.dot(W2)\n",
    "    I3 = Z3 + b2\n",
    "    A3 = softmax(I3)\n",
    "\n",
    "    # print '   b2: %s' % str(b2)\n",
    "    # print 'Z3[0]: %s' % str(Z3[0])\n",
    "    # print 'I3[0]: %s' % str(I3[0])\n",
    "    # print 'A3[0]: %s' % str(A3[0])\n",
    "    # print ' y[0]: %s' % str(labels[0])\n",
    "\n",
    "    C = np.sum(A3 * labels, 1) #start create cost vector per training example\n",
    "    C = np.log(C)\n",
    "\n",
    "    cost = -1*np.sum(C) / m\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "\n",
    "    D3 = A3 - labels\n",
    "\n",
    "    # print '*** shapes ***'\n",
    "    # print \"A2: %s\" % str(A2.shape)\n",
    "    # print \"D3: %s\" % str(D3.shape)\n",
    "    # print \"W2: %s\" % str(W2.shape)\n",
    "\n",
    "    dW2 = A2.T.dot(D3)\n",
    "\n",
    "    S2 = sigmoid_grad(A2)\n",
    "\n",
    "    D2 = S2 * D3.dot(W2.T)\n",
    "    # print \"S2: %s\" % str(S2.shape)\n",
    "    # print \"D2: %s\" % str(D2.shape)\n",
    "\n",
    "    dW1 = data.T.dot(D2)\n",
    "\n",
    "    gradW2 = dW2 / m\n",
    "    gradb2 = D3.sum(axis=0) / m\n",
    "    gradW1 = dW1 / m\n",
    "    gradb1 = D2.sum(axis=0) / m\n",
    "\n",
    "    # print('*** final ***')\n",
    "    # print \"gradW2 %s\" % str(gradW2.shape)\n",
    "    # print \"gradb2 %s\" % str(gradb2.shape)\n",
    "    # print \"gradW1 %s\" % str(gradW1.shape)\n",
    "    # print \"gradb1 %s\" % str(gradb1.shape)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### Stack gradients (do not modify)\n",
    "\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "#forward_backward_prop(data, labels, params)\n",
    "\n",
    "# Perform gradcheck on your neural network\n",
    "#print \"=== For autograder ===\"\n",
    "gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2vec\n",
    "\n",
    "*Please answer the third complementary problem before starting this part.*\n",
    "\n",
    "In this part you will implement the `word2vec` models and train your own word vectors with stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement your skip-gram and CBOW models here\n",
    "\n",
    "# Interface to the dataset for negative sampling\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, assuming the softmax prediction function and cross      #\n",
    "    # entropy loss.                                                   #\n",
    "    # Inputs:                                                         #\n",
    "    #   - predicted: numpy ndarray, predicted word vector (\\hat{r} in #\n",
    "    #           the written component)                                #\n",
    "    #   - target: integer, the index of the target word               #\n",
    "    #   - outputVectors: \"output\" vectors for all tokens              #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: cross entropy cost for the softmax word prediction    #\n",
    "    #   - gradPred: the gradient with respect to the predicted word   #\n",
    "    #           vector                                                #\n",
    "    #   - grad: the gradient with respect to all the other word       # \n",
    "    #           vectors                                               #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    probabilities = softmax( outputVectors.dot(predicted) )\n",
    "    t = np.zeros(len(probabilities)) # target index -> one hot target vector\n",
    "    t[target] = 1\n",
    "\n",
    "    #see written answers for explanation\n",
    "    cost = - np.log(probabilities[target])\n",
    "    gradPred = - outputVectors[target] + outputVectors.T.dot(probabilities)\n",
    "    grad = np.outer(probabilities - t, predicted)\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, K=10, printCosts=False):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, using the negative sampling technique. K is the sample  #\n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  #\n",
    "    # a random word index.                                            #\n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    samples = np.zeros((K+1, outputVectors.shape[1]))\n",
    "    samples[0] = outputVectors[target]\n",
    "    indices = [target]\n",
    "    for i in range(1, K+1):\n",
    "#        neg = target\n",
    "#        while neg == target:\n",
    "        neg = dataset.sampleTokenIdx()\n",
    "\n",
    "        samples[i] = outputVectors[neg]\n",
    "        indices.append(neg)\n",
    "\n",
    "    product = samples.dot(predicted)\n",
    "\n",
    "    sig = sigmoid(product)\n",
    "    cost = - np.log(sig[0]) - np.sum( np.log(1 - sig)[1:] )\n",
    "\n",
    "    sig[0] -= 1\n",
    "    gradPred = samples.T.dot(sig)\n",
    "\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "\n",
    "    # vectorized\n",
    "#    sample_errs = np.outer(sig, predicted)\n",
    "#    for i in range(0,K+1):\n",
    "#        idx = indices[i]\n",
    "#        grad[idx] += sample_errs[i]\n",
    "\n",
    "    # unvectorized style\n",
    "    for i in range(0,K+1):\n",
    "        idx = indices[i]\n",
    "        grad[idx] += sig[i] * predicted\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    if printCosts:\n",
    "        print cost\n",
    "        print gradPred\n",
    "        print grad\n",
    "        print '========================='\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the skip-gram model in this function.                 #         \n",
    "    # Inputs:                                                         #\n",
    "    #   - currentWord: a string of the current center word           #\n",
    "    #   - C: integer, context size                                    #\n",
    "    #   - contextWords: list of no more than 2*C strings, the context #\n",
    "    #             words                                               #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - inputVectors: \"input\" word vectors for all tokens           #\n",
    "    #   - outputVectors: \"output\" word vectors for all tokens         #\n",
    "    #   - word2vecCostAndGradient: the cost and gradient function for #\n",
    "    #             a prediction vector given the target word vectors,  #\n",
    "    #             could be one of the two cost functions you          #\n",
    "    #             implemented above                                   #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: the cost function value for the skip-gram model       #\n",
    "    #   - grad: the gradient with respect to the word vectors         #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    cost = 0\n",
    "    \n",
    "    #for each context word, calculate the cost, cost function gradients and add them up to get \n",
    "    #the final, combined cost and gradients\n",
    "    for i in range(0, len(contextWords)):\n",
    "        target = tokens[contextWords[i]] \n",
    "        predicted_idx = tokens[currentWord] # x_i\n",
    "        predicted = inputVectors[predicted_idx] #v_c\n",
    "        cost_i, gradPredicted, gradOut_i = word2vecCostAndGradient(predicted, target, outputVectors)\n",
    "        cost += cost_i\n",
    "        gradIn[predicted_idx] += gradPredicted\n",
    "        gradOut += gradOut_i\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the continuous bag-of-words model in this function.   #         \n",
    "    # Input/Output specifications: same as the skip-gram model        #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    cost = 0\n",
    "    gradIn = 0\n",
    "    gradOut = 0\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Implement a function that normalizes each row of a matrix to have unit length\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    n = np.linalg.norm(x, axis=1)\n",
    "    n = n.reshape(len(n), 1) # so the normalization occurs row wise\n",
    "    x = x / n\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test this function\n",
    "print \"=== For autograder ===\"\n",
    "print normalizeRows(np.array([[3.0,4.0],[1, 2]]))  # the result should be [[0.6, 0.8], [0.4472, 0.8944]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "\n",
      "=== For autograder ===\n",
      "(16.514833617181822, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-4.91591439, -2.01855734,  1.12480697],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.88442555,  0.40583834,  3.08711651],\n",
      "       [-0.12708467,  0.05831563,  0.44359323],\n",
      "       [-0.45528438,  0.20891737,  1.58918512],\n",
      "       [-0.42136814,  0.19335415,  1.47079939],\n",
      "       [-0.64496237,  0.29595533,  2.25126239]]))\n",
      "(16.34741310225029, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-5.39056903, -2.5789063 ,  0.76565988],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.69148188,  0.31730185,  2.41364029],\n",
      "       [ 0.07307589, -0.0335325 , -0.25507379],\n",
      "       [-0.68292656,  0.31337605,  2.38377767],\n",
      "       [-0.42136814,  0.19335415,  1.47079939],\n",
      "       [-0.80620296,  0.36994417,  2.81407799]]))\n"
     ]
    }
   ],
   "source": [
    "# Gradient check!\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "print \"==== Gradient check for skip-gram ====\"\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "print \"\\n==== Gradient check for CBOW      ====\"\n",
    "#gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "#gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "#print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)\n",
    "print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)\n",
    "#print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "#print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement SGD\n",
    "\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the stochastic gradient descent method in this        #\n",
    "    # function.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - f: the function to optimize, it should take a single        #\n",
    "    #        argument and yield two outputs, a cost and the gradient  #\n",
    "    #        with respect to the arguments                            #\n",
    "    #   - x0: the initial point to start SGD from                     #\n",
    "    #   - step: the step size for SGD                                 #\n",
    "    #   - iterations: total iterations to run SGD for                 #\n",
    "    #   - postprocessing: postprocessing function for the parameters  #\n",
    "    #        if necessary. In the case of word2vec we will need to    #\n",
    "    #        normalize the word vectors to have unit length.          #\n",
    "    #   - PRINT_EVERY: specifies every how many iterations to output  #\n",
    "    # Output:                                                         #\n",
    "    #   - x: the parameter value after SGD finishes                   #\n",
    "    ###################################################################\n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        ### YOUR CODE HERE\n",
    "        ### Don't forget to apply the postprocessing after every iteration!\n",
    "        ### You might want to print the progress every few iterations.\n",
    "        \n",
    "        cost, grad = f(x)\n",
    "        x = x - step*grad\n",
    "        x = postprocessing(x)\n",
    "        \n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            print '*** iteration %d: cost = %f' % (iter, cost)\n",
    "            #print 'x = ' + str(x) \n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show time! Now we are going to load some real data and train word vectors with everything you just implemented!**\n",
    "\n",
    "We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load some data and initialize word vectors\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** iteration 100: cost = 19.824035\n",
      "*** iteration 200: cost = 22.568762\n",
      "*** iteration 300: cost = 18.299645\n",
      "*** iteration 400: cost = 20.738883\n",
      "*** iteration 500: cost = 23.941542\n",
      "*** iteration 600: cost = 21.653786\n",
      "*** iteration 700: cost = 23.636321\n",
      "*** iteration 800: cost = 19.215293\n",
      "*** iteration 900: cost = 19.061025\n",
      "*** iteration 1000: cost = 24.550921\n",
      "*** iteration 1100: cost = 23.786120\n",
      "*** iteration 1200: cost = 20.430142\n",
      "*** iteration 1300: cost = 21.350372\n",
      "*** iteration 1400: cost = 24.083680\n",
      "*** iteration 1500: cost = 19.050782\n",
      "*** iteration 1600: cost = 19.666508\n",
      "*** iteration 1700: cost = 20.711840\n",
      "*** iteration 1800: cost = 21.321447\n",
      "*** iteration 1900: cost = 23.424069\n",
      "*** iteration 2000: cost = 21.589960\n",
      "*** iteration 2100: cost = 19.923260\n",
      "*** iteration 2200: cost = 18.978924\n",
      "*** iteration 2300: cost = 21.629459\n",
      "*** iteration 2400: cost = 22.239242\n",
      "*** iteration 2500: cost = 20.587114\n",
      "*** iteration 2600: cost = 18.008551\n",
      "*** iteration 2700: cost = 18.349916\n",
      "*** iteration 2800: cost = 18.748163\n",
      "*** iteration 2900: cost = 19.685194\n",
      "*** iteration 3000: cost = 20.398693\n",
      "*** iteration 3100: cost = 21.302148\n",
      "*** iteration 3200: cost = 19.937245\n",
      "*** iteration 3300: cost = 22.172635\n",
      "*** iteration 3400: cost = 18.663886\n",
      "*** iteration 3500: cost = 23.252472\n",
      "*** iteration 3600: cost = 19.601855\n",
      "*** iteration 3700: cost = 21.181658\n",
      "*** iteration 3800: cost = 17.548129\n",
      "*** iteration 3900: cost = 19.351662\n",
      "*** iteration 4000: cost = 19.083676\n",
      "*** iteration 4100: cost = 18.841555\n",
      "*** iteration 4200: cost = 18.795914\n",
      "*** iteration 4300: cost = 19.414534\n",
      "*** iteration 4400: cost = 17.192191\n",
      "*** iteration 4500: cost = 16.181429\n",
      "*** iteration 4600: cost = 19.836444\n",
      "*** iteration 4700: cost = 17.172912\n",
      "*** iteration 4800: cost = 17.853492\n",
      "*** iteration 4900: cost = 16.346469\n",
      "*** iteration 5000: cost = 16.552720\n",
      "*** iteration 5100: cost = 16.973532\n",
      "*** iteration 5200: cost = 20.714942\n",
      "*** iteration 5300: cost = 16.698444\n",
      "*** iteration 5400: cost = 17.280867\n",
      "*** iteration 5500: cost = 16.110255\n",
      "*** iteration 5600: cost = 15.879479\n",
      "*** iteration 5700: cost = 15.706166\n",
      "*** iteration 5800: cost = 17.161365\n",
      "*** iteration 5900: cost = 13.811571\n",
      "*** iteration 6000: cost = 15.163102\n",
      "*** iteration 6100: cost = 17.468855\n",
      "*** iteration 6200: cost = 12.634784\n",
      "*** iteration 6300: cost = 15.333054\n",
      "*** iteration 6400: cost = 15.422822\n",
      "*** iteration 6500: cost = 15.312256\n",
      "*** iteration 6600: cost = 15.288912\n",
      "*** iteration 6700: cost = 13.643749\n",
      "*** iteration 6800: cost = 14.464991\n",
      "*** iteration 6900: cost = 16.636697\n",
      "*** iteration 7000: cost = 14.216064\n",
      "*** iteration 7100: cost = 14.949226\n",
      "*** iteration 7200: cost = 14.158379\n",
      "*** iteration 7300: cost = 15.004758\n",
      "*** iteration 7400: cost = 15.235412\n",
      "*** iteration 7500: cost = 13.681580\n",
      "*** iteration 7600: cost = 13.768510\n",
      "*** iteration 7700: cost = 14.923230\n",
      "*** iteration 7800: cost = 14.681832\n",
      "*** iteration 7900: cost = 13.148248\n",
      "*** iteration 8000: cost = 15.203188\n",
      "*** iteration 8100: cost = 13.732643\n",
      "*** iteration 8200: cost = 14.052102\n",
      "*** iteration 8300: cost = 12.968285\n",
      "*** iteration 8400: cost = 13.553750\n",
      "*** iteration 8500: cost = 15.451216\n",
      "*** iteration 8600: cost = 11.765293\n",
      "*** iteration 8700: cost = 15.660313\n",
      "*** iteration 8800: cost = 11.078927\n",
      "*** iteration 8900: cost = 12.428679\n",
      "*** iteration 9000: cost = 12.514261\n",
      "*** iteration 9100: cost = 11.889638\n",
      "*** iteration 9200: cost = 13.195022\n",
      "*** iteration 9300: cost = 14.174796\n",
      "*** iteration 9400: cost = 13.091693\n",
      "*** iteration 9500: cost = 12.303028\n",
      "*** iteration 9600: cost = 12.444862\n",
      "*** iteration 9700: cost = 12.471380\n",
      "*** iteration 9800: cost = 13.022372\n",
      "*** iteration 9900: cost = 13.811607\n",
      "*** iteration 10000: cost = 12.766225\n",
      "*** iteration 10100: cost = 13.419790\n",
      "*** iteration 10200: cost = 14.241720\n",
      "*** iteration 10300: cost = 12.982380\n",
      "*** iteration 10400: cost = 11.553540\n",
      "*** iteration 10500: cost = 12.852635\n",
      "*** iteration 10600: cost = 11.505490\n",
      "*** iteration 10700: cost = 12.496512\n",
      "*** iteration 10800: cost = 12.567788\n",
      "*** iteration 10900: cost = 12.621255\n",
      "*** iteration 11000: cost = 12.724441\n",
      "*** iteration 11100: cost = 11.413386\n",
      "*** iteration 11200: cost = 12.011073\n",
      "*** iteration 11300: cost = 12.633332\n",
      "*** iteration 11400: cost = 11.820355\n",
      "*** iteration 11500: cost = 12.427883\n",
      "*** iteration 11600: cost = 11.208838\n",
      "*** iteration 11700: cost = 10.553970\n",
      "*** iteration 11800: cost = 13.440380\n",
      "*** iteration 11900: cost = 11.267851\n",
      "*** iteration 12000: cost = 11.726927\n",
      "*** iteration 12100: cost = 11.689890\n",
      "*** iteration 12200: cost = 11.975908\n",
      "*** iteration 12300: cost = 13.599944\n",
      "*** iteration 12400: cost = 12.744723\n",
      "*** iteration 12500: cost = 9.984948\n",
      "*** iteration 12600: cost = 12.161424\n",
      "*** iteration 12700: cost = 11.442586\n",
      "*** iteration 12800: cost = 11.808564\n",
      "*** iteration 12900: cost = 10.517031\n",
      "*** iteration 13000: cost = 14.038131\n",
      "*** iteration 13100: cost = 12.140934\n",
      "*** iteration 13200: cost = 12.221101\n",
      "*** iteration 13300: cost = 12.210837\n",
      "*** iteration 13400: cost = 10.715014\n",
      "*** iteration 13500: cost = 9.522793\n",
      "*** iteration 13600: cost = 11.705700\n",
      "*** iteration 13700: cost = 11.797467\n",
      "*** iteration 13800: cost = 10.159226\n",
      "*** iteration 13900: cost = 11.914864\n",
      "*** iteration 14000: cost = 11.654085\n",
      "*** iteration 14100: cost = 8.858871\n",
      "*** iteration 14200: cost = 9.534999\n",
      "*** iteration 14300: cost = 11.355053\n",
      "*** iteration 14400: cost = 12.751844\n",
      "*** iteration 14500: cost = 11.330817\n",
      "*** iteration 14600: cost = 11.875633\n",
      "*** iteration 14700: cost = 10.745395\n",
      "*** iteration 14800: cost = 11.921761\n",
      "*** iteration 14900: cost = 9.919528\n",
      "*** iteration 15000: cost = 10.583920\n",
      "*** iteration 15100: cost = 11.258435\n",
      "*** iteration 15200: cost = 10.523780\n",
      "*** iteration 15300: cost = 12.152353\n",
      "*** iteration 15400: cost = 10.300107\n",
      "*** iteration 15500: cost = 9.993344\n",
      "*** iteration 15600: cost = 11.048485\n",
      "*** iteration 15700: cost = 9.863507\n",
      "*** iteration 15800: cost = 10.471859\n",
      "*** iteration 15900: cost = 11.250309\n",
      "*** iteration 16000: cost = 10.598477\n",
      "*** iteration 16100: cost = 11.191369\n",
      "*** iteration 16200: cost = 10.785642\n",
      "*** iteration 16300: cost = 10.125308\n",
      "*** iteration 16400: cost = 10.579227\n",
      "*** iteration 16500: cost = 11.027564\n",
      "*** iteration 16600: cost = 10.207828\n",
      "*** iteration 16700: cost = 10.494012\n",
      "*** iteration 16800: cost = 10.616674\n",
      "*** iteration 16900: cost = 11.010110\n",
      "*** iteration 17000: cost = 9.783098\n",
      "*** iteration 17100: cost = 11.031762\n",
      "*** iteration 17200: cost = 10.116074\n",
      "*** iteration 17300: cost = 11.440438\n",
      "*** iteration 17400: cost = 9.938228\n",
      "*** iteration 17500: cost = 9.792252\n",
      "*** iteration 17600: cost = 10.768696\n",
      "*** iteration 17700: cost = 11.442431\n",
      "*** iteration 17800: cost = 11.922240\n",
      "*** iteration 17900: cost = 9.016028\n",
      "*** iteration 18000: cost = 9.520071\n",
      "*** iteration 18100: cost = 10.582530\n",
      "*** iteration 18200: cost = 10.822322\n",
      "*** iteration 18300: cost = 12.051097\n",
      "*** iteration 18400: cost = 9.332144\n",
      "*** iteration 18500: cost = 9.928644\n",
      "*** iteration 18600: cost = 10.572174\n",
      "*** iteration 18700: cost = 11.153395\n",
      "*** iteration 18800: cost = 10.724766\n",
      "*** iteration 18900: cost = 9.455426\n",
      "*** iteration 19000: cost = 11.047075\n",
      "*** iteration 19100: cost = 9.542766\n",
      "*** iteration 19200: cost = 10.581598\n",
      "*** iteration 19300: cost = 9.970339\n",
      "*** iteration 19400: cost = 9.007167\n",
      "*** iteration 19500: cost = 10.374346\n",
      "*** iteration 19600: cost = 11.386380\n",
      "*** iteration 19700: cost = 10.126053\n",
      "*** iteration 19800: cost = 9.615098\n",
      "*** iteration 19900: cost = 11.783063\n",
      "*** iteration 20000: cost = 10.151595\n",
      "*** iteration 20100: cost = 9.515311\n",
      "*** iteration 20200: cost = 9.901412\n",
      "*** iteration 20300: cost = 9.759898\n",
      "*** iteration 20400: cost = 10.041742\n",
      "*** iteration 20500: cost = 8.990106\n",
      "*** iteration 20600: cost = 8.496027\n",
      "*** iteration 20700: cost = 10.117156\n",
      "*** iteration 20800: cost = 11.350651\n",
      "*** iteration 20900: cost = 10.327365\n",
      "*** iteration 21000: cost = 10.384174\n",
      "*** iteration 21100: cost = 10.755044\n",
      "*** iteration 21200: cost = 10.326444\n",
      "*** iteration 21300: cost = 9.919119\n",
      "*** iteration 21400: cost = 10.450458\n",
      "*** iteration 21500: cost = 9.159895\n",
      "*** iteration 21600: cost = 9.265591\n",
      "*** iteration 21700: cost = 10.059992\n",
      "*** iteration 21800: cost = 8.675887\n",
      "*** iteration 21900: cost = 10.684042\n",
      "*** iteration 22000: cost = 10.692417\n",
      "*** iteration 22100: cost = 11.000593\n",
      "*** iteration 22200: cost = 9.675748\n",
      "*** iteration 22300: cost = 9.191926\n",
      "*** iteration 22400: cost = 10.019881\n",
      "*** iteration 22500: cost = 9.841838\n",
      "*** iteration 22600: cost = 10.695031\n",
      "*** iteration 22700: cost = 10.134393\n",
      "*** iteration 22800: cost = 10.130307\n",
      "*** iteration 22900: cost = 8.583705\n",
      "*** iteration 23000: cost = 7.977067\n",
      "*** iteration 23100: cost = 9.962668\n",
      "*** iteration 23200: cost = 9.941753\n",
      "*** iteration 23300: cost = 9.632672\n",
      "*** iteration 23400: cost = 8.711986\n",
      "*** iteration 23500: cost = 9.666737\n",
      "*** iteration 23600: cost = 9.213794\n",
      "*** iteration 23700: cost = 9.859836\n",
      "*** iteration 23800: cost = 10.957001\n",
      "*** iteration 23900: cost = 9.804074\n",
      "*** iteration 24000: cost = 10.431906\n",
      "*** iteration 24100: cost = 10.324138\n",
      "*** iteration 24200: cost = 10.373589\n",
      "*** iteration 24300: cost = 9.654841\n",
      "*** iteration 24400: cost = 11.675356\n",
      "*** iteration 24500: cost = 9.166465\n",
      "*** iteration 24600: cost = 9.170704\n",
      "*** iteration 24700: cost = 9.496887\n",
      "*** iteration 24800: cost = 10.311302\n",
      "*** iteration 24900: cost = 10.112697\n",
      "*** iteration 25000: cost = 10.990664\n",
      "*** iteration 25100: cost = 11.561296\n",
      "*** iteration 25200: cost = 8.973448\n",
      "*** iteration 25300: cost = 8.258680\n",
      "*** iteration 25400: cost = 11.195232\n",
      "*** iteration 25500: cost = 8.496003\n",
      "*** iteration 25600: cost = 10.470967\n",
      "*** iteration 25700: cost = 10.549611\n",
      "*** iteration 25800: cost = 11.222353\n",
      "*** iteration 25900: cost = 9.862382\n",
      "*** iteration 26000: cost = 9.220387\n",
      "*** iteration 26100: cost = 10.623430\n",
      "*** iteration 26200: cost = 11.271656\n",
      "*** iteration 26300: cost = 9.398806\n",
      "*** iteration 26400: cost = 10.041322\n",
      "*** iteration 26500: cost = 9.601602\n",
      "*** iteration 26600: cost = 9.442383\n",
      "*** iteration 26700: cost = 9.763220\n",
      "*** iteration 26800: cost = 10.212975\n",
      "*** iteration 26900: cost = 10.493176\n",
      "*** iteration 27000: cost = 9.350890\n",
      "*** iteration 27100: cost = 10.342700\n",
      "*** iteration 27200: cost = 9.061780\n",
      "*** iteration 27300: cost = 9.275926\n",
      "*** iteration 27400: cost = 9.143075\n",
      "*** iteration 27500: cost = 9.281768\n",
      "*** iteration 27600: cost = 10.135935\n",
      "*** iteration 27700: cost = 10.333916\n",
      "*** iteration 27800: cost = 9.314976\n",
      "*** iteration 27900: cost = 9.401295\n",
      "*** iteration 28000: cost = 10.119991\n",
      "*** iteration 28100: cost = 9.446550\n",
      "*** iteration 28200: cost = 9.057067\n",
      "*** iteration 28300: cost = 10.537179\n",
      "*** iteration 28400: cost = 9.257551\n",
      "*** iteration 28500: cost = 10.066162\n",
      "*** iteration 28600: cost = 9.448552\n",
      "*** iteration 28700: cost = 11.038767\n",
      "*** iteration 28800: cost = 10.723536\n",
      "*** iteration 28900: cost = 9.075501\n",
      "*** iteration 29000: cost = 10.106888\n",
      "*** iteration 29100: cost = 9.726801\n",
      "*** iteration 29200: cost = 8.389442\n",
      "*** iteration 29300: cost = 9.201874\n",
      "*** iteration 29400: cost = 9.454006\n",
      "*** iteration 29500: cost = 10.404515\n",
      "*** iteration 29600: cost = 9.129010\n",
      "*** iteration 29700: cost = 9.003618\n",
      "*** iteration 29800: cost = 9.259082\n",
      "*** iteration 29900: cost = 9.169905\n",
      "*** iteration 30000: cost = 10.308157\n",
      "*** iteration 30100: cost = 8.404398\n",
      "*** iteration 30200: cost = 9.246759\n",
      "*** iteration 30300: cost = 9.162554\n",
      "*** iteration 30400: cost = 8.320600\n",
      "*** iteration 30500: cost = 9.733194\n",
      "*** iteration 30600: cost = 10.198696\n",
      "*** iteration 30700: cost = 10.150765\n",
      "*** iteration 30800: cost = 11.228881\n",
      "*** iteration 30900: cost = 10.873820\n",
      "*** iteration 31000: cost = 8.886681\n",
      "*** iteration 31100: cost = 8.742483\n",
      "*** iteration 31200: cost = 11.629306\n",
      "*** iteration 31300: cost = 9.842549\n",
      "*** iteration 31400: cost = 9.450381\n",
      "*** iteration 31500: cost = 8.690105\n",
      "*** iteration 31600: cost = 8.194436\n",
      "*** iteration 31700: cost = 10.320902\n",
      "*** iteration 31800: cost = 9.718504\n",
      "*** iteration 31900: cost = 10.865541\n",
      "*** iteration 32000: cost = 8.543699\n",
      "*** iteration 32100: cost = 8.639180\n",
      "*** iteration 32200: cost = 10.274402\n",
      "*** iteration 32300: cost = 8.513340\n",
      "*** iteration 32400: cost = 7.891284\n",
      "*** iteration 32500: cost = 9.585316\n",
      "*** iteration 32600: cost = 9.653370\n",
      "*** iteration 32700: cost = 9.880459\n",
      "*** iteration 32800: cost = 9.032324\n",
      "*** iteration 32900: cost = 9.278436\n",
      "*** iteration 33000: cost = 9.430437\n",
      "*** iteration 33100: cost = 9.944981\n",
      "*** iteration 33200: cost = 10.206665\n",
      "*** iteration 33300: cost = 9.843640\n",
      "*** iteration 33400: cost = 9.789134\n",
      "*** iteration 33500: cost = 10.188897\n",
      "*** iteration 33600: cost = 10.119359\n",
      "*** iteration 33700: cost = 10.063484\n",
      "*** iteration 33800: cost = 9.983002\n",
      "*** iteration 33900: cost = 9.043369\n",
      "*** iteration 34000: cost = 9.937092\n",
      "*** iteration 34100: cost = 8.880352\n",
      "*** iteration 34200: cost = 10.258279\n",
      "*** iteration 34300: cost = 9.808698\n",
      "*** iteration 34400: cost = 9.285778\n",
      "*** iteration 34500: cost = 8.930630\n",
      "*** iteration 34600: cost = 10.106471\n",
      "*** iteration 34700: cost = 10.339554\n",
      "*** iteration 34800: cost = 9.832233\n",
      "*** iteration 34900: cost = 9.664235\n",
      "*** iteration 35000: cost = 10.403163\n",
      "*** iteration 35100: cost = 9.612732\n",
      "*** iteration 35200: cost = 8.117835\n",
      "*** iteration 35300: cost = 8.685601\n",
      "*** iteration 35400: cost = 8.329807\n",
      "*** iteration 35500: cost = 9.269226\n",
      "*** iteration 35600: cost = 10.782543\n",
      "*** iteration 35700: cost = 10.149704\n",
      "*** iteration 35800: cost = 9.348094\n",
      "*** iteration 35900: cost = 9.602455\n",
      "*** iteration 36000: cost = 10.227238\n",
      "*** iteration 36100: cost = 10.435614\n",
      "*** iteration 36200: cost = 9.907020\n",
      "*** iteration 36300: cost = 10.362604\n",
      "*** iteration 36400: cost = 9.227624\n",
      "*** iteration 36500: cost = 8.496729\n",
      "*** iteration 36600: cost = 10.614879\n",
      "*** iteration 36700: cost = 9.498869\n",
      "*** iteration 36800: cost = 8.056495\n",
      "*** iteration 36900: cost = 8.938755\n",
      "*** iteration 37000: cost = 10.730534\n",
      "*** iteration 37100: cost = 10.257204\n",
      "*** iteration 37200: cost = 8.731385\n",
      "*** iteration 37300: cost = 9.332523\n",
      "*** iteration 37400: cost = 9.281973\n",
      "*** iteration 37500: cost = 9.228365\n",
      "*** iteration 37600: cost = 10.698982\n",
      "*** iteration 37700: cost = 11.707178\n",
      "*** iteration 37800: cost = 9.738782\n",
      "*** iteration 37900: cost = 9.591843\n",
      "*** iteration 38000: cost = 9.161417\n",
      "*** iteration 38100: cost = 9.910537\n",
      "*** iteration 38200: cost = 9.364862\n",
      "*** iteration 38300: cost = 7.691761\n",
      "*** iteration 38400: cost = 11.367483\n",
      "*** iteration 38500: cost = 10.888157\n",
      "*** iteration 38600: cost = 10.371942\n",
      "*** iteration 38700: cost = 9.612289\n",
      "*** iteration 38800: cost = 7.979291\n",
      "*** iteration 38900: cost = 9.604600\n",
      "*** iteration 39000: cost = 9.600471\n",
      "*** iteration 39100: cost = 8.772975\n",
      "*** iteration 39200: cost = 9.264063\n",
      "*** iteration 39300: cost = 10.005296\n",
      "*** iteration 39400: cost = 9.428453\n",
      "*** iteration 39500: cost = 8.713349\n",
      "*** iteration 39600: cost = 10.483383\n",
      "*** iteration 39700: cost = 9.830625\n",
      "*** iteration 39800: cost = 9.246245\n",
      "*** iteration 39900: cost = 10.547286\n",
      "*** iteration 40000: cost = 10.071870\n",
      "\n",
      "=== For autograder ===\n",
      "[[-0.42228835 -0.27681538  0.10529904  0.57622994  1.03184906 -0.48800304\n",
      "   0.04397824  0.61315279 -0.8191535  -0.09451214]\n",
      " [-0.45905296 -0.16120535  0.09131196  0.48465452  0.95757914 -0.45019033\n",
      "  -0.07439518  0.52200259 -0.82881761 -0.18574232]\n",
      " [-0.31640883 -0.00899457  0.01107082  0.43816862  0.8405119  -0.23609758\n",
      "  -0.01563366  0.42258454 -0.51017922 -0.07996252]\n",
      " [-0.38121623 -0.17627352  0.06691346  0.41548791  0.78141731 -0.28542256\n",
      "  -0.00521071  0.52342752 -0.53175287 -0.04917845]\n",
      " [-0.15802201 -0.00561573 -0.0538053   0.15367163  0.23874534 -0.04897271\n",
      "  -0.04566645  0.10129293 -0.07614706 -0.04504949]\n",
      " [-0.37856394 -0.14663006  0.04081689  0.50195396  0.75466956 -0.31939618\n",
      "  -0.0263935   0.40979692 -0.50570645 -0.07874919]\n",
      " [-0.35960172 -0.15939     0.07154291  0.46451714  0.89994815 -0.29895742\n",
      "  -0.10805682  0.5456585  -0.66393962 -0.15666107]]\n"
     ]
    }
   ],
   "source": [
    "# Train word vectors (this could take a while!)\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / dimVectors, \n",
    "                              np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, negSamplingCostAndGradient), \n",
    "                   wordVectors, 0.3, 40000, None, True, PRINT_EVERY=100)\n",
    "# sanity check: cost at convergence should be around or below 10\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "checkWords = [\"the\", \"a\", \"an\", \"movie\", \"ordinary\", \"but\", \"and\"]\n",
    "checkIdx = [tokens[word] for word in checkWords]\n",
    "checkVecs = wordVectors[checkIdx, :]\n",
    "print checkVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.10423499281960046, 0.17447891706738722)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAl4AAAHiCAYAAAA5wcIVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmcVXX9x/HXd1aGUcQpcEFCU4w0RS23LEQFxCU1c+On\n",
       "paZG+tNU1FxBBLdUzGwxt1xK0VwjlxRMNEsR+SmYAiEqKiogA4JsM8N8f3/MZRpgNrgz5y68no8H\n",
       "es+533PO51x5zLz9nu/9fkOMEUmSJLW/gkwXIEmStKEweEmSJCXE4CVJkpSQokwXIEmSsk/oGnpQ\n",
       "QclqOyupinPjrAyVlBcMXpIkaW0VlDCIJavtG015hqrJGz5qlCRJSojBS5IkKSEGL0mSpIQYvCRJ\n",
       "khJi8JIkSUqIwUuSJCkhBi9JkqSEGLwkSVLzruRBnqNrpsvIByHGmOkamhVCyO4CJUnKR8VAByAC\n",
       "S4ByYAVQncmickOMMTT1Xk7MXN/cDSQhhDA8xjg8kzXkMj+/9PkZps/PMD1+funLtc8w9Ao9GcQS\n",
       "/sz2vMexnM9IRlMep8UZGaspBz7DljqMciJ4SZKkDDmG/wAjM11GvnCMlyRJUkLs8Wqd8ZkuIMeN\n",
       "z3QBeWB8pgvIA+MzXUCOG5/pAvLA+EwXsE4qqVprUexKqjJUzSrjM3z9tOXE4PpMj/GSJElqjZZy\n",
       "i48aJUmSEmLwkiRJSojBS5IkKSEGL0mSpIQYvCRJkhJi8JIkSUqIwUuSJCkhBi9JkqSEGLwkSZIS\n",
       "YvCSJElKiMFLkiQpIQYvSZKkhBi8JEmSEmLwkiRJSojBS5IkKSEGL0mSpIQYvCRJkhJi8JIkSUqI\n",
       "wUuSJCkhBi9JkqSEGLwkSZISYvCSJElKiMFLkiQpIQYvSZKkhBi8JEmSEmLwkiRJSojBS5IkKSEG\n",
       "L0mSpIQYvCRJkhKSdvAKIQwMIUwLIcwIIVzYyPu9QggvhxCWhxDOW+O990MIU0IIr4cQXk23FkmS\n",
       "pGxWlM7BIYRC4DdAP2A2MDGEMCbGOLVBs/nAWcARjZwiAn1jjJXp1CFJkpQL0u3x2gN4J8b4foyx\n",
       "GngAOLxhgxjjvBjja0B1E+cIadYgSZKUE9INXt2ADxtsf5Ta11oRGBdCeC2EcFqatUiSJGW1tB41\n",
       "Uhec0rFPjPGTEEIXYGwIYVqM8R9pnlOSJCkrpRu8ZgPdG2x3p67Xq1VijJ+k/j0vhPAYdY8u1wpe\n",
       "IYThDTbHxxjHr0+xkiRJbSmE0Bfo2+r2Ma5/p1UIoQiYDhwAfAy8CgxaY3D9qrbDgcUxxlGp7Y5A\n",
       "YYxxcQihHHgWuCLG+Owax8UYo+PAJElS1mspt6TV4xVjrAkhnAk8AxQCd8YYp4YQBqfevzWEsDkw\n",
       "EegE1IYQzgZ2ALoCj4YQVtVx35qhS5IkKZ+k1eOVBHu8JElSrmgptzhzvSRJUkIMXpIkSQkxeEmS\n",
       "JCXE4CVJkpQQg5ckSVJCDF6SJEkJMXhJkiQlxOAlSZKUEIOXJElSQgxekiRJCTF4SZIkJcTgJUmS\n",
       "lBCDlyRJUkIMXpIkSQkxeEmSJCXE4CVJkpQQg5ckSVJCDF6SJEkJMXhJkiQlxOAlSZKUEIOXJElS\n",
       "QgxekiRJCTF4SZIkJcTgJUmSlBCDlyRJUkIMXpIkSQkxeEmSJCXE4CVJkpQQg5ckSVJCDF6SJEkJ\n",
       "MXhJkiQlxOAlSZKUEIOXJElSQgxekiRJCTF4SZIkJcTgJUmSlBCDlyRJUkIMXpIkSQkxeEmSJCXE\n",
       "4CVJkpQQg5ckSVJCDF6SJEkJMXhJkiQlxOAlSZKUEIOXJElSQgxekiRJCSnKdAFNCV1DDyoooRhC\n",
       "r9Az0/W0SiVVcW6clekyJElSdsra4EUFJQxiCaOAQSzJdDmtMpryTJcgSZKyl48aJUmSEmLwamgk\n",
       "j2e6BEmSlL8MXg0N5YhMlyBJkvJX9o7xaugq7qSWLYmUsgl3cDb3M5wZdORulnMABczlK1zHB1xK\n",
       "LVvQlcv5KWN5kq14nZuJdATgK1zKiUzies5nOf0BqOXLlDCeizmP4cxgOD25k735hPMoYD419KKI\n",
       "KVzCWQDcyv7M4XICSynhNWrozqWclKmPRpIk5Y7c6PEawBCGchAncDCfcwoT6AyU0ZmXGMb+BL7g\n",
       "Ay7gZxzN9pzCPM4H4Ot8xkkcx1AGshOn8wEjAbiAGxjKgRzKUQQW0IO7UleK9desYUf2YBgXsy8r\n",
       "+Qp38y1mUcqn/IJv8T8M5SBWUrHaMZIkSc3IjR6v8ZzK0xwIQC1bMJ1tgCp+wgsAlDCVAlbQiVoO\n",
       "YxrX0R2Azynmb1xFNTsQWMlKtq0/ZzXwFL+hM7fyP/x7rWsW8Qb9mANAMW+xmO78i2UUMouDmQ1A\n",
       "BY8zn+Pb8c4lSVIeyf4erxpgOd/hRL7HMAZQxFvUUJp6Z5VIoBqAjkRWBcpx/IRC5jCMfpzNQUBx\n",
       "/RE3cR6FzOZnPNTodQMrGmzVEili7d6tkN7NSZKkDUn2By+AAhbSgxU8zHbUsFurj6tlI4qYB8Dd\n",
       "HA0UAvB7+rOc73IKw1p9rkDk28xkJT14im4AVHIYPmqUJEmtlP2PGguBGooYwXgKmUkRk1LvrBl4\n",
       "4lqve3EPk7mdERxFB8ZDaiLWzziNWjbj9zwJQAee5QJGNXqOhnqwgs24mNe4n0kspZg3qDV4SZKk\n",
       "1gkxZmduCL1Cz9TM9bM5L9XDlA2mUUYvlgFwLVdRwrsM4U4ARlMep8UZmSxPkiRlTgghxhibHIqU\n",
       "/T1e2eYpjuchjiZSQjFv8gP+lOmSJElSbjB4rash3AHckekyJElS7sne4FVJFaMpZzm5s/h0JVWZ\n",
       "LkGSJGWvrB3jtUpLz0olSZKyRUu5JTemk5AkScoDBi9JkqSEGLwkSZISYvCSJElKiMFLkiQpIQYv\n",
       "SZKkhBi8JEmSEmLwkiRJSojBS5IkKSFpB68QwsAQwrQQwowQwoWNvN8rhPByCGF5COG8dTlWkiQp\n",
       "n6S1ZFAIoRCYDvQDZgMTgUExxqkN2nQBegBHAAtijKNae2yqnUsGSZKknNDeSwbtAbwTY3w/xlgN\n",
       "PAAc3rBBjHFejPE1oHpdj5UkScon6QavbsCHDbY/Su1r72MlSZJyTrrBa/2fU6Z3rCRJUs4pSvP4\n",
       "2UD3Btvdqeu5atNjQwjDG2yOjzGOb32JkiRJ7SOE0Bfo2+r2aQ6uL6JugPwBwMfAqzQyQD7Vdjiw\n",
       "uMHg+lYd6+B6SZKUK1rKLWn1eMUYa0IIZwLPAIXAnTHGqSGEwan3bw0hbE7dNxY7AbUhhLOBHWKM\n",
       "XzR2bDr1SJIkZbO0erySYI+XJEnKFe09nYQkSZJayeAlSZKUEIOXJElSQgxekiRJCTF4SZIkJcTg\n",
       "JUmSlBCDlyRJUkIMXpIkSQkxeEmSJCXE4CVJkpQQg5ckSVJCDF6SJEkJMXhJkiQlxOAlSZKUEIOX\n",
       "JElSQgxekiRJCTF4SZIkJcTgJUmSlJCiTBcgSZKaFrqGHlRQ0myjSqri3DgroZKUBoOXJEnZrIIS\n",
       "BrGk2TajKU+oGqXJR42SJEkJscdLkiS1qVY9Hl0XefQo1eAlSZLaVmsej66LPHqUavCSJClXXMWd\n",
       "1LIlkVI24Q7O5v5Ml6R14xgvSZJyxQCGMJSDOIGD+ZxTmEDnTJfUrNfZmJv4EQB3sjdXcXdmC8o8\n",
       "e7wkScoV4zmVpzkQgFq2YDrbsCevZ7iqpn3MJiziRODeTJeSLQxekiTlgjvZm+V8hxP5Hj1YwZU8\n",
       "RE0bDmBvD5O5hFq2ZiTPADUElnI1t1JDL4qYwiWcBcB97MR7XE6knEAl+3AO+zEvs8W3Dx81SpKU\n",
       "C6rZmAIW0oMVPMK21LBbpktq0S5cRQHvM5QD6cZIavgGezCMi9mXlXyFu9mdeRTxLldyAKcxlIPY\n",
       "lAd5hYsyXXp7yakerzb/emom5NFXYiVJCTqc5/kDP2QE4ylkJkVMynRJLYqE1V4X8Qb9mANAMW+x\n",
       "mK14nkWs5GuM4wHGAZFCClJt8lBOBa82/3pqJuTRV2IlSQnagmou5YeZLiMtgRUNtmqJqRxSyHSG\n",
       "cnhmikqWjxolSVL7qGAJkY2afD8Q2Y+ZRL7EvalHp/Mo4iF6JlVi0nKrxyvT7mMnPuJoLmRYpkuR\n",
       "JCnr7c0CnmciI3iOwHIKmLtWmy7U0JOfMJORjGBjoIhNuA2YkXi9CQgxxkzX0KwQQowxBoDQK/TM\n",
       "h0eNcVrMy79MkqS216rxzVk2frjNf1/n0O/OhrmlMbnb49XY7L3DmUE5t7OMfgSWsy8n04f5XMMv\n",
       "KWAxVexMpCtf5krO4CmqgV9yGcvZD4h8iV/xv/yVa7iJzjzF6TwLwNX8hgrGUMxiPmUwl3IS13Me\n",
       "NWzJSr5CLd3YmNs5l7sAuIFzWMqRBOZTyMd0YApDuDVzH5YkKVdlU6BS+nJ3jFfjs/eW0YlJDGMA\n",
       "pUzgVY6vb7+SLgzlCHbgR3zGJQDczsFUsSMXcwB9OZbPGMrzdGEzRlPJsUDdrLvVfJMfMXatGmrY\n",
       "lsEMoj+HsIjzWEQBf6Q3yziI0ziAH3A8NewMZHe3oiRJSkTu9ng1NnsvVDGY5wAoZwqL6JNqHenE\n",
       "3wA4incYThcAFrMHnXiMYqAP8/knLzOdXfgpYxnBNfyTTZnIoZTxBB3XCk+RjoyjCzV0YQFj+YwJ\n",
       "dGUeu9OBZ9iCaragmlLGAk12OUqSlHcqqWrTb/FXUtVm58qw3Axejc/eWwrU1LcJ1AKF9dsFVDc4\n",
       "w6ogFFk9FAVW9U6V8zATOIolHMYOnNNoHWG1c66kmsImzilJ0gbDx6NNy81HjQ1n732Y7dZ79t5O\n",
       "TGARh7GUwEtUUMWe7JBa82p3HmQxpwGRHzCzkaPXDlSBSFcmspz+fEQJb9ORFRyAjxolSRK5GrwO\n",
       "53mgiBGMZzoXN5i9t2HAiY1sr/76dP5GCW9zA+P4O3/my4ykD/OBukePhfyHTjxYf1RY7Zxrnr/O\n",
       "CUyhA8/yB8bxKH+iiGkUsiit+5UkSXnB6SSaMoMOjOY5vs8AdlrHa06jjF4sYwYdeIBH2Y4LGMRb\n",
       "QE59JVaSJK2b/J1Ooj3dwXeZzQ104tZ1Dl0Aj3I9K+lJpAPlPFgfuiRJ0gbNHq+k2eMlSVLeyq8e\n",
       "r7b+emom5NFXYiVJ0rrJqR4vSZKkbNZSbsnNbzVKkiTlIIOXJElSQgxekiRJCTF4SZIkJcTgJUmS\n",
       "lBCDlyRJUkIMXpIkSQkxeEmSJCXE4CVJkpSQ3FoyKIeErqEHFZRkug4qqYpz46xMlyFJkgxe7aeC\n",
       "kqxY0DvX17aUJCmP+KhRkiQpIQav9vYkWzGC59b7+Os5jzv4ThtWJEmSMsRHjdlsKYELGJXpMiRJ\n",
       "UtsweCWjiKv5NTXsRBHTOY6z+Tvf4hOGEimkmMmcxEVsQTVXMIEy/sIK+lDB71jEfmzCWM7gKa5g\n",
       "Ah35M8vpT6SIbzCYI5nJS1Qwnt9RS1eKmUQVfRjIgezJwkzfuCRJ+i8fNSahlm3pxt0Moy+BL3iU\n",
       "wczml+zMYIbRDyhkND9KtY4UUslQBvK/jAFi6k/de0XMZygD6cS9TOenALzEEMp4kWHsz5d4gki3\n",
       "5G9SkiS1xOCVhMDHnMgkALryCMvZh0JmcTjvA9CFh1jGXvXtd2RMk+famacAqOBNVtIdgGp25+v8\n",
       "BYCf8ALBni5JkrKRwSsZscGrQGAREBq8HwgN2mzK0ibPtBFVABSwEihc7bySJCmrGbySEOnGvewG\n",
       "wDy+TymTWUl3xtAjte8HlPHyep+/mIlM4zAAbqcPkc7pFy1Jktqawau9BSIFzGQ2JzGC8UQ25mhu\n",
       "oxvnMpnbGME4YCUn8MfUEbG506123lVtv8ONLKMPI3iO+RxKYC5f5Yt2uR9JkrTeQoyt+z2fKSGE\n",
       "GGPMucdooVfomdjM9Z9QTDkr6UQt9/BNPuBqhnIgAKMpj9PijETqkCRpA9dSbnE6iXzwKt2Ywq2p\n",
       "8WPVfJXzM12SJElam8ErHxzO+xye6uGSJElZy+DVXiqpyooFqitT34KUJEkZ5xgvSZKkNtJSbvFb\n",
       "jZIkSQkxeEmSJCXE4CVJkpSQtINXCGFgCGFaCGFGCOHCJtrcnHp/cghh1wb73w8hTAkhvB5CeDXd\n",
       "WiRJkrJZWt9qDCEUAr8B+gGzgYkhhDExxqkN2hwMbBdj7BlC2BO4BeoXhI5A3xhjZTp1ZIPQNfSg\n",
       "gpJM19FmKqmKc+OsTJchSVI+SXc6iT2Ad2KM7wOEEB4ADgemNmhzGHAPQIxxQgihcwhhsxjjnNT7\n",
       "+fGNxQpKEpupPgnZMBWGJEl5Jt1Hjd2ADxtsf5Ta19o2ERgXQngthHBamrVIkiRltXR7vFo7CVhT\n",
       "vVrfiTF+HELoAowNIUyLMf5jrYNDGN5gc3yMcfy6lZlHhjOD4fRcp2N+y6HM53wKmMNlHNtkuyuY\n",
       "wEAOZE8WplumJEkbghBCX6Bva9unG7xmA90bbHenrkeruTZbpfYRY/w49e95IYTHqHt0uVbwijEO\n",
       "T7POfNL6GW+rgWoCCxjEVzifk3itzc4tSZJIdQaNX7UdQri8ufbpBq/XgJ4hhK2Bj4FjgUFrtBkD\n",
       "nAk8EELYC1gYY5wTQugIFMYYF4cQyoEBwBVp1pN5ozidApZzLnfxC4ZTzde5jGO5g32Yy3F0YhyV\n",
       "nAUEOvAcP+dqoK4nq5zbWUY/AsvZl5Ppw3yeoDuv81siHSnj2bWutZRDiZRQxt+4gFE8yVZM4n6K\n",
       "+T+q2ZmO/JUaducDbuQ6nqED/2EpvbmIywC4invYgt/xYyYk/VFJkrShSWuMV4yxhrpQ9QzwNvBg\n",
       "jHFqCGFwCGFwqs1TwLshhHeAW4EzUodvDvwjhPAGMAF4Isb47FoXyTVdeIVl7AlAFb2JdKSSQhaw\n",
       "JyW8y3wupT9Hcw79qaI3tzAgdWQZnZjEMAZQygRe5XgAJjOCztzNMPpRzJz669xOH6rZmqEcwvkM\n",
       "oIqduIs9AKhlG7biboaxP+fzS4qYzLacwc+5qpGK7eWSJCkhaS+SHWN8Gnh6jX23rrF9ZiPHvQvs\n",
       "ku71s85BvMkt7MyblBNYQTGTeYLerGAPynmWEv7J3iwAoBOPsYi9gGeBKgbzHADlTGERfQCoYXdO\n",
       "5BQA9uMRHuNSACrZlxXsy0ieASDSkcVsQ1c+JvARP+SNZG9ckiS1JO3gpTV0oYYCPmAcx1DKRMqZ\n",
       "ylz2YSVbU8ZHLGfn+raxwT+hpn5/oBYobPFanfk1Z3PfavueZCsCS5s8JlBDwy87RDq0fFOSJKkt\n",
       "uGRQeyjlVRZxOpvyCt9gAkv4EUW8ydd4nSr25mU2ZREFLOYINuGVZs9VxETu4XAAnufI+v0VjGcR\n",
       "xzGNMgDGsjkvUdFibRvzEdXsSDXwN7akJg97HSVJylIGr/awKROIdGF/JtGH+QSWUcYE9mMeX+Jq\n",
       "xvIQNzGWEibzU8amjmo41irWb+/CUBZyEiMYRzWb1e8/jX+wEY/xZ/7KCMbxCr9nYf2kp02P2zqJ\n",
       "iRTyIdfwApMYQRFT2vr2JUlS40KM2T22OoQQY4xZP7t96BV65tvM9XFanJHpMiRJyiUt5RZ7vCRJ\n",
       "khJi8JIkSUqI32psK5VU5dXC0pVUZbqEfBe6hh5UUNKmJ62kKs6Ns9r0nJKkNmPwaiP+stM6q6Ck\n",
       "zccF5lP4l6Q85KNGKdOeZCtGpCbPTfJYSVLiDF6SJEkJ8VGjlB2KuJpfU8NOFDGd4zibRzmdZfQj\n",
       "0oESXuMiLgTgPnZiJjdSt+7AC5ktW5K0LuzxkrJBLdvSjbsZRl8CX/A4J7Ivf2AohzCMA4h04Pf0\n",
       "A2Amv6Q7lzCsfoF1SVKOsMdLygaBjzmRSQB05RE+5RQm8iF/4wwiHYh0ZgnTmcSrRDpxMhMB6MEj\n",
       "TGf/TJYuSWo9e7yk7BAbvApAZB5XsxenMox+dOR+IqUUrLEcVCTrV3WQJP2XwUvKBpFu3MtuAMzj\n",
       "+5TxKgDbs4C36cgyDgVgVxYT+Jy72R2ADxosnC5Jyno+apQyLRApYCazOYkR3EgR0zmCe3mETbiH\n",
       "vxOYRzGv17fflnOZyY2MJFLKizS3KLokKau4SLaUIe2ysLqLm0tSRrlItiRJUpYweEmSJCXE4CVJ\n",
       "kpQQB9dLmVJJVZsval1JVZueT5LUphxcL0mS1EYcXC9JkpQlDF6SJEkJMXhJkiQlxOAlSZKUEIOX\n",
       "JElSQgxekiRJCTF4SZIkJcQJVJX3QtfQgwpK2uXklVTFuXFWu5xbkpR3DF7KfxWUMIgl7XLutp55\n",
       "XpKU13zUKLXkSh7mPr4BwBVMYAKdM1yRJClHGbyklsUmXkuStE581KgNxyhOp4DlnMtd/ILhVPN1\n",
       "LuNY7mAf5nIcXfgzn3I+UEoh73Mk59KLZZkuW5KUPwxeeapdB5S3RjYOOu/CK3zEYOAuqugNFFFJ\n",
       "IQvYk1KmModzOI5j6clyRnEGTzCYXtyU6bIlSfnD4JWv2nNAeWtk46Dzg3iTW9iZNyknsIJiJvME\n",
       "vVnBHpTzDCvZngf4CwCREop5LcMVS5LyjMFLG44u1FDAB4zjGEqZSDlTmcs+rGRryviQZbzIJfxv\n",
       "psuUJOUvB9dvaK7nPG5kcNrnqfum305tUFGySnmVRZzOprzCN5jAEn5EEW+yE5OoZnfG0AOAaZTx\n",
       "ONtkuFpJUp6xx2vD01bfysvNb/dtygSWcBb7M4mvspzxLKOMCezDAqZxDpP5HW+kxsZ15RfAe5kt\n",
       "WJKUT0KM2f37M4QQY4wh03XkmtAr9Kwf43UDP2MpRxGYTyEf04EpLKUf2zCC43mTf7Ip43iay9mL\n",
       "mzmGxQwkUsZKtqETt1JLKUv5PrCCgfyQ3fmcK3mIYt6mir2IFLENQ/ghk+sLGE15nBZnZOj2V7Pa\n",
       "Z9HWsug+JUmZ11Ju8VFjvruPnVjKYZxIf47gBGro3eDdxlP3SrbnKH5MPw7icy6kiMUM5UBKmMQ/\n",
       "OCrVKhDpwFAOpDsX8x43tvu9SJKU4wxe+W4Oe1LG0/RgBTuxhFKebfGYEv5FL5axDwsILOIbjAWg\n",
       "jGlU0z3VKtKFxwE4mVeJbMxkNmqv25AkKR8YvPJfU8+SV1JLIQBf0GGN96oavK5lo/rtWmIz4wIL\n",
       "cnTclyRJCXFwfb7bnAm8wy+ZxW9YRBEr6E8xf6KQD5nPzsBk3uKQVp4trPZ6HocBL3MXuxP4nJ0y\n",
       "OG9Ycyqpard5xSpXC6mSJDXL4JXv/od/cwNjuIexBOZTxBtA5Gv8nn/ze0ZyAh0Yx397xiKhybUJ\n",
       "G74XCaxgJM8QKeSrDEnidtZH1s2gL0naYPmtxjzVrt/kaw2/7SdJ2gD5rUZJkqQsYfCSJElKiGO8\n",
       "8lV7Dihv7fUlSdJqHOMlSZLURhzjJUmSlCUMXpIkSQkxeEmSJCXE4CVJkpQQg5ckSVJCDF6SJEkJ\n",
       "MXhJkiQlxOAlSZKUEIOXJElSQgxekiRJCTF4SZIkJcRFsjMkdA09qKCkXU5eSVWcG2e1y7klSdJ6\n",
       "M3hlSgUlDGJJu5x7NOXtcl5JkpQWHzVKkiQlxOCVD67nPG5kcKbLkCRJzTN45YeY6QIkSVLLHOOV\n",
       "DX7NUSxgMBAp5m16cz3/xy+pZVMKmM+3GMJAPuZJtuJ1blxr/yov0o1iSoCOoVfI2O00y4H/kqQN\n",
       "mMEr0/7M9izgZxzIYezJQiayCc/yKzbhQc7iEW7mWCYxkoGcwhtc2ej+VYopYW+WUgrs1k4D99Pl\n",
       "wH9J0gbMR42ZNpt9KOOv7MlCAHbnc2r4JifyGACDeIQa9gBocv+aPmBzRvBci9e+nvO4g30AuJKH\n",
       "uY9vAHAFE5hAZwBG8vh639vNHM1zdF3v4yVJyjP2eGWHtZ8L1jSyr7n962opgQsY1WBPbPT1UI5Y\n",
       "72ss4hg+ZRowd73PIUlSHjF4ZdpWvMRU7mQCt7EnC5lAZ4p4jfs4nLN4lNEcSRGvADS5v7HgBkVc\n",
       "za+pYSeKmM5xnM0feYEy/sIK+lDB71jEfmzCWM7gqSbrG84MhtOTt+nIo/yBSGciRXTlOn7KszzJ\n",
       "VkziPkqYQBXfooBP+SEn8yT9qKE3M/ktI1nGjziMHqxo+w9QkqTckfajxhDCwBDCtBDCjBDChU20\n",
       "uTn1/uQQwq7rcmzeO5oZbMrNPMMjjOBZ/s5QduNSPudYRjCWRRzJtxgG0OT+uh6q1b/ZWMu2dONu\n",
       "htGXwBc8xklApJBKhjKQ/2VMo8etre79rVjO4ZzCUAZyAEczt/7aUMvWbM1dDGN/CvicMRzMGTxJ\n",
       "EZPZljMYyoGGLkmS0uzxCiEUAr8B+gGzgYkhhDExxqkN2hwMbBdj7BlC2BO4BdirNcduMM7iYeDh\n",
       "1fYdxLFrtTuIjxvdfwE3AvAy29TvC3zMiUwCoCuP8CmnArAjY9arxhUU8CQX8xh7Eqills15kS8B\n",
       "UMAHHEfdf7dS3mQF3RscmaVfr5QkKXnp9njtAbwTY3w/xlgNPAAcvkabw4B7AGKME4DOIYTNW3ms\n",
       "1l9s8CoAtQBsytL1OttojqSWCs7hQIZyIIHPWEJp6t2q+naBlUBho3VIkrSBSzd4dQM+bLD9UWpf\n",
       "a9ps2Ypjtb4i3biX3QCYx/cp49W0zreSjShkPp2o5U6+TWSrZq5d18sV+ILlbJzWdSVJyiPpDq5v\n",
       "bW9GWo+bQgjDG2yOjzGOT+d8WaGSqnaY06ojpUA1VRQwk9mcxAhupIjpHMG9/JEfr8c56/4b78Vj\n",
       "jONuRjCOYiZTwIy12qwSUtub8mdm8wsH10uS8lUIoS/Qt9XtY1z/J0EhhL2A4THGganti4HaGOMv\n",
       "GrT5PXVh6YHU9jRgX2Cblo5N7Y8xRscJtULoFXoyKEsnTl1lNOVxWpzRckNJknJPS7kl3R6v14Ce\n",
       "IYStgY+BY4FBa7QZA5wJPJAKagtjjHNCCPNbcay0ltA19KCCkkQv6lJHkqQ2kFbwijHWhBDOBJ6h\n",
       "bkD1nTHGqSGEwan3b40xPhVCODiE8A6wBDi5uWPTqUcbiApKEu/Zc6kjSVIbSHsC1Rjj08DTa+y7\n",
       "dY3tM1t7rCRJUr5y5vp80j4D9ttWZYOpJ7LVLQygC+9yFO9kuhRJUn4xeOURxyA1YhEFdErNYdZa\n",
       "CzmIyFjRthT9AAAaT0lEQVQweEmS2pbBS7ntBs5hKUcSmE8hH9OBKSylH8W8RRV7UM5jbMYrvMfl\n",
       "RMoJVLIP57Af8/gV/8MijidSTBHvcxw/40W+QRX9mcdejORsduE0vscHmb5NSVJ+MHgpd/2R3izj\n",
       "IE7jABZQzMM8A0wBIFLEUA6mkkJ+y6P04yT2ZgG/5TBe4SL24zz24in25H4ArucC/sIgzuUuruHZ\n",
       "1OLhjj+UJLUpg5dy1zx2pwPPsAXVbEE1pYytf2+z1JqU49iOlXyNcTzAOOqWCS9gDgBv0YtnuZDI\n",
       "xkTKKeX5Bmd37jhJUpszeCmXRZoKSCWpNSkjgUKmM7SRdUA/5CZ6cRLHMo2bOZol7L3GuSVJalPp\n",
       "rtUoZU5XJrKc/nxECW/TkRX0a/BuXSDrw0wiX2qwbmURD9Ez1aacbsxlHkUs4gesClsFLKHGNSYl\n",
       "SW3PHi/lrhOYwvU8yx8YRwGfUcQ0CllEXYCqC1FbUE1PfsJMRjKCjYEiNuE2YAabcB3P8STPM58S\n",
       "XqeWjgBsxl/4gOsZyY/ZhZ84uF6S1FbSWqsxCa7VqDWttiblNMroxTJm0IEHeJTtuIBBvNXmF3WN\n",
       "SUlSK7T3Wo1SZj3K9aykJ5EOlPNgu4QuSZLaiMFLue0SGl2OSpKkbGTwUu7JxNJIubDUkSQp6znG\n",
       "S5IkqY20lFucTkKSJCkhPmqUcljoGnpQQUmrGldS5ULqkpRZBi8pl1VQUj+1RkuSHhcnSVqLwUvK\n",
       "Vy/SjeLVesM6hl7tNFzS3jRJahWDl5Sviilh79SalQClwG6t7B1bV/amSVKrOLhekiQpIQYvSevu\n",
       "SbZiBM9lugxJyjUGL0mSpIQ4xkvaEPyW06jkEJ7kUwr5mA5MYQte4h2uJdKBQmYxkCF8k0WMZsdG\n",
       "99/HTszkRiDSgRcyfUuSlIvs8ZLywU2cyEieYSTPcCWjGckzvMwFADzFDixkfw7gR/yA46lhZwBm\n",
       "cBNbMZJh9KeEqYxjSGr/rxrdP5Nf0p1LGMaAzNykJOU+e7ykfHAO9wD3rLbvZbYB4EN2YROepwM1\n",
       "7MBSShlLLR2JbMLJvApALx7iDW5jMhsR6bTW/tfZOLV/IgA9eITp7J/cDUpSfjB4bcDWadbz9uL8\n",
       "T0mIQPMTeMUm3l/X/ZKkZhm8NmTrMut5e3H+p/bXncm8zqUs4wHepiMr6MdG3EdgIXexOyczkekc\n",
       "RQn/ojdf8JdG9u/KYv7K59zN7pzERD7gyEzfliTlIoOXlO8O5m3e5QX+zj0EPqGIaRTyOT05h3e4\n",
       "lhGUUcgsDuZcgCb3b8u5zORGRhIp5UXqetIkSevA4KW2dQsD6MK7HMU7mS5FDfyAe/mEP7Exn/AA\n",
       "j9KVNxnE28Bha7Vtav/x/BtWG1h/VXuVK0n5yuCltrWQg4iMBYNXIiqpauZxbUdKU6+eZRjVbE2k\n",
       "gHIeZBBvJVWiJOm/DF6CUZxOAcs5l7v4BcOp5utcxrHcwT7M5TgKWEw1uxDpQBlPcgGjALiOS1hO\n",
       "f6CGDrzAZjxNFf2Zx16M5Gx25VRqKWAKVxH5EoFl7MAFHMnMzN5w/mjuiwmhV/jv2oy7cXJiRUmS\n",
       "mmTwEnThFT5iMHAXVfQGiqikkAXsSTkv822eZHc+ZxEF3MyDPEgvvsIcljOQYfQBYDIb0ZsvuIZn\n",
       "2YSxnMHTAFzJg+zEhRzO+9zLrrzN1RzJsZm72Q1I871hbX8tSVKLDF6Cg3iTW9iZNyknsIJiJvME\n",
       "vVnBHmzDZfyTw/gbxxMpJLIZn9GTg/gPY1nONYyiM2M5inENzlg31cDbdKSGbzGFW5mSeidmePqK\n",
       "DYjTdEhS9jF4CbpQQwEfMI5jKGUi5UxlLvuwkq0pYzmLGMz3OIhdWcw13MhKOtCJWk7jEMbwHSo5\n",
       "lFs5mcvqe7Lqvu1WTQGBzxnKgZm7OUmSsofBS3VKeZVFnE43zmV7pvE8V1DEGyxmY2Ap32Ax4/ky\n",
       "VezPRvyLaZQxl44M5nle5zXG8DIABSyhho0B6M0XjOEDfschnMGTVAOP8HWOY2rmblS5bp0n/nWS\n",
       "XklZxLUaVWdTJhDpwv5Mog/zCSyjjAkcx1SK+TfX8CIv8RuKU0vJfMJGvMA9jGAsf+UxKrgcgM34\n",
       "Cws5nZH8jSfozm6cSSWDGMGzXMPzfOQ6f0rTqol/D2Q5M7mNIrZsdHvVn0yvziBJDYQYs3sOxBBC\n",
       "jDG6PEk7CL1Cz2yYuT5OizMyWoNyymp/b1+ighcYxemcSgUr67fP4hQ6UQv4d0xSolrKLT5qVF7I\n",
       "+LqTPs7KjO9QyXcaTJWx5rYkZRmDl/JDpteddM1JSVIrGLw2ZEnO89RcDZIkbSAMXhswH42tpz+w\n",
       "F4VUcyKTMl2KJCm3GLykdTWffSjkCzB4SZLWjcFL+WUaZTzCraxkc6CQch5hObtyKadxCwOYwy38\n",
       "jO1ZTBH38DzD+DZj6NHoepIvUcGLXMtKugHQncvpyics5QRgJSM5kq24jJOZmNF7liTlDIOX8svf\n",
       "2Y9CPuVSfgTUrSH5F04AYBF7UshUxrArtRRRxP8BMIXrGl1P8kVGsiW3cxITeZotmcj9nERf3uKP\n",
       "FPIFQ7gtY/cpScpJBi/lly2ZyjyGcR2X0IWxnMxE/sosHmFbqtmFCm5jPnsSKaQjE5hGWZPrSVbz\n",
       "XT5kO0bW7y9nGmWpLeeWkyStM4OX8ssRvEd3BvAvDmA2F3IDL9GBV5jFAUA1u/APxvMrIgVszwhW\n",
       "UNjMepKBUzmULahebe9fE7kTSVIecskg5Zfn6MqXWcFZPMaX+T0r2IkKJrCY0yjlNfZhAbVsSi1f\n",
       "5Rj+Q2++oCC1niRANfAAXwegmBe4n1Pqzz2aHQEo5AtWslHi9yZJynn2eCm/vEsv/slQArVADV/l\n",
       "QvbmHe7lS2zKBACKeZuVdKk/ZjfO5HWuZQRnA8V05HFgKvsxlOe5ihGMBYoo4WXgEr7KWKZwGyMZ\n",
       "QHcu4yQH10uSWse1GpUXMr7upOsBJmadl4dyOSdJCXKtRkl5xRAlKZc5xkuSJCkh9ngpP2R63UnX\n",
       "nJQktYJjvCRJktpIS7nFR42SJEkJMXhJkiQlxOAlSZKUEIOXJElSQgxekiRJCTF4SZIkJcTgJUmS\n",
       "lBCDlyRJUkIMXpIkSQkxeEmSJCXE4CVJkpQQg5ckSVJCDF6SJEkJMXhJkiQlxOAlSZKUkKJMFyBl\n",
       "s9A19KCCkowVUElVnBtnZez6kqQ2ZfCSmlNBCYNYkrHrj6Y8Y9eWJLU5HzVKkiQlxOAlJelGTuVd\n",
       "OtRvD2dGBquRJCVsvYNXCKEihDA2hPCfEMKzIYTOTbQbGEKYFkKYEUK4sMH+4SGEj0IIr6f+DFzf\n",
       "WqScsIgCFnMqcyhrsDdmrB5JUuLSGeN1ETA2xnhdKlBdlPpTL4RQCPwG6AfMBiaGEMbEGKdS9wvn\n",
       "xhjjjWnUICVjFKdTwHLO5S5+wXCq+TqXcSx3sA9zOY5OjKOSs4BAB57j51wN1PVolXEvK/guHXmK\n",
       "yGaM5SGeYz6XcSwA1/NzltGPwHL25WT6MD+DdypJakfpPGo8DLgn9foe4IhG2uwBvBNjfD/GWA08\n",
       "ABze4P2QxvWl5HThFZaxJwBV9CbSkUoKWcCelPAu87mU/hzNOfSnit7cwoDUkWVszP8xjAGcz00E\n",
       "5nAgR9WHLuhIJyYxjAGUMoFXOT4j9ydJSkQ6wWuzGOOc1Os5wGaNtOkGfNhg+6PUvlXOCiFMDiHc\n",
       "2dSjSikrHMSb1LAzb1JOYAUlTOIJerOCPSjkc0r4J3uzgE7U0onHWMReqSNXchpPNnPmKgbzHADl\n",
       "TKGa7u1+L5KkjGn2UWMIYSyweSNvXdpwI8YYQwiNjVVpbvzKLcCI1OuRwCjglCbqGN5gc3yMcXwz\n",
       "55XaXhdqKOADxnEMpUyknKnMZR9WsjVlfMRydq5vGxv8E1ZQ3OyZa+pfBWqBwrYuXZLUfkIIfYG+\n",
       "rW3fbPCKMfZv5kJzQgibxxg/DSFsAcxtpNlsWO3/4LtT1+tFjLG+fQjhDuCvzdQxvLk6pUSU8iqL\n",
       "OJ1unMv2TON5rqCIN/gar/MiI3mZTdmRz1nMEXyZOxs9R+ALPmMjYGGyxUuS2kOqM2j8qu0QwuXN\n",
       "tU/nUeMY4MTU6xOBxxtp8xrQM4SwdQihBDg2dRypsLbK94E306hFan+bMoFIF/ZnEn2YT2AZZUxg\n",
       "P+bxJa5mLA9xE2MpYTI/ZWzqqNV7fTfmPiZxP1fyYCPvx7XaS5LySohx/X7OhxAqgD8DXwHeB46J\n",
       "MS4MIWwJ3B5jPCTV7iDgJuoeodwZY7wmtf9eYBfqftG8BwxuMGas4XVijNFB+MqI0Cv0zPTM9XFa\n",
       "dK4vScoRLeWW9Q5eSTF4KZMMXpKkddFSbnGtRrWLjC8u3Xa25kUW0ofZmS5EkpT7DF5qH5leXLqt\n",
       "vMJC3qMzozM0GL6SqoxcV5LULgxeUnP2YjbvsdDHfZKktuAi2couN3ECv+YHbXrOK3mY+9hprf03\n",
       "cwzXcmWbXkuSpGbY46Xscg5/aoezNvUNkuz+ZokkKe8YvNT+fs2RLOTHQDHFvM4ZXMKNTKec29da\n",
       "HPp6zqOQLxjCrYxmR97hWiIdKGQWAxnCbDZlMrcylIEAPM42vMktDGUgN3Auy+hHpAMlvMZFXFhf\n",
       "w0ccxUhuIFLENgzhh0xercaXqOBFrmVlakmr7lzOSbyW2GckSdog+KhR7ethtmMR3+NsDmMoBwIr\n",
       "uYcjgbImFof+7ySiM/gVWzGSYfSnhKmMYwiHMYvAIkazAwD/4Vg24gEA9uUPDOUQhnEAkQ78nn6p\n",
       "cwYiHRjKgXTnYt7jxvr9q7zISLbkdoZyCN/iND7ghvb+aCRJGx57vNS+PuS71LAzv+JpgFTv1XzW\n",
       "XBx6EX1WO24yGxHpxMm8CkAvHuINbgOgM/fzAceylOEs53vsx8EATGQf/sbpRMqIdGYJ04FxQKRL\n",
       "amWFk3mVK9iY19l4tetV810+ZDtGprYj5cygAz1Z3vYfiiRpQ2XwUvsr4yF+zrWr7RvOT+tft2Zx\n",
       "6Nigd+p7PM09DOFe/kkRU9idz5lFKfO4mm8zkP58yvUMIVLa5PmKqF1jT+BUDmULqlt9X5IkrSMf\n",
       "Nap9decfLOcQXqICgAl05qnUOKrm9OYLAgu5i90BmM5RlPAvAHqwglJeYA7X8OXUY8bPUiFrexbw\n",
       "Nh1ZxqENzhaYx2EA3MXuBD5npzXmGCvmBe7nlPrt0ey4fjcsSVLT7PFS+zqKd/gt1/E8o/k7BQSq\n",
       "6cGltGZx6J6cwztcywjKKGQWB3Nu/Xtb8BjvMZCTeQGAb7KIv3Mf9/B3AvMo5vXVzh9YwUieIVLI\n",
       "VxnSYH/ddfdjKM9zFSMYCxRRwsvAJW34SUiS5FqNah/rvcbhtVxJRybzMx5qtt0ofkot5VzAqPWt\n",
       "sdVcL1GS1Equ1ajccT0XUE1v9ub6ZttdxZ2spDsHckxClUmS1Cbs8VK7WO8er2y0AfZ4tbjIeSVV\n",
       "cW6clWBJkpQT7PFSZlRSxWjKM11Gm9gQF6puaZHzfPlvK0kJM3ipXdgbIknS2pxOQpIkKSEGL0lN\n",
       "u4JXAHiSrbiyhW+aSpJaZPCSJElKiMFLUtMK+AyAQmopYEGGq5GknOd0EpLW0uJ0IBvgFBuS1Bot\n",
       "5RZ7vCRJkhJi8JIkSUqIwUuSJCkhBi9JkqSEGLwkSZISYvCSJElKiGs1SlpbS4ucb4gLh0tSG3Ae\n",
       "L0mSpDbiPF6SJElZwuAlSZKUEIOXJElSQgxekiRJCTF4SZIkJcTpJCRt8ELX0IMKStbr4Eqq4tw4\n",
       "q41LkpSnDF6SVEEJg1iyXsc2N9+ZJK3BR42SJEkJMXhJkiQlxOAlSZKUEMd4SVJDV3EntWxJpJRN\n",
       "uIOzuZ/hzKCc21lGPwLL2ZeT6cP8TJcqKffY4yVJDQ1gCEM5iBM4mM85hQl0BsroxCSGMYBSJvAq\n",
       "x2e6TEm5yR4vSWpoPKfyNAcCUMsWTGcboIrBPAdAOVNYRJ8MVigph9njJUmr3MneLOc7nMj3GMYA\n",
       "iniLGkqBmvo2gVqgMGM1SsppBi9JWqWajSlgIT1YwcNsRw27ZbokSfnF4CVJqxzO80ARIxjPdC6m\n",
       "iEmpd2KDVnGNbUlqtRBjdv/8CCHEGGPIdB2S8lfoFXqmM3N9nBZntHFJknJUS7nFHi9JkqSEGLwk\n",
       "SZIS4nQSklRJ1Xovdl1JVRtXIymPOcZLkiSpjTjGS5IkKUsYvCRJkhJi8JIkSUqIwUuSJCkhBi9J\n",
       "kqSEGLwkSZISYvCSJElKiMFLkiQpIQYvSZKkhBi8JEmSEmLwkiRJSojBS5IkKSEGL0mSpIQYvCRJ\n",
       "khJi8JIkSUqIwUuSJCkhBi9JkqSEGLwkSZISYvCSJElKiMFLkiQpIUWZLkAShK6hBxWUrLazkqo4\n",
       "N87KUEmSpHaw3sErhFABPAj0AN4HjokxLmyk3R+AQ4C5Mcad1vV4aYNQQQmDWLLavtGUZ6gaSVI7\n",
       "SedR40XA2Bjj9sBzqe3G3AUMTON4SWkIXUOP0Cv0bPJP19Aj0zVKUq5b9bOWYmju52s6jxoPA/ZN\n",
       "vb4HGE8j4SnG+I8Qwtbre7y0Qbme8yjkC4Zwa5uds7HetIbsWZOk9K36WTsK6n/mNvLzNZ0er81i\n",
       "jHNSr+cAmyV8vJSPYqYLkCS1n2Z7vEIIY4HNG3nr0oYbMcYYQljvXxgtHR9CGN5gc3yMcfz6Xkva\n",
       "YD3CtrzNKCIbUcBCDuA09mZBpsuSpFwWQugL9KWMCm6huqX2zQavGGP/Zi40J4SweYzx0xDCFsDc\n",
       "day11cfHGIev47klrSkAu3Emh/AR13ER/+SH7M3NmS5LknJZqjNofOgVeqYeNQ5prn06jxrHACem\n",
       "Xp8IPJ7w8VK+ap/HjUcyk0P4KHWFUgpY3i7XkSQ1KZ3gdS3QP4TwH2D/1DYhhC1DCE+uahRCGA38\n",
       "C9g+hPBhCOHk5o7PRqluRK0nP791cAE3MoTb1tzdpp/hbezLCvbjAO5vs3PmAP8epsfPL31+hunL\n",
       "h89wvb/VGGOsBPo1sv9j6ubtWrU9aF2Oz1J9qfvWpdZPX/z80tWXtvgMlxL4hBvY9f/bu//Qu+o6\n",
       "juPPF2NhKmOK4dwPUXMNM5w/0MRJfKWCNW1FjcpaWgpKuLQQayr4h6HoH9IoRWSKKEISKrVgzEYZ\n",
       "SZgw2A9Mv7T5C1dtS2c/TM0NX/5xj3r3/d7v7kfv7rnn7r4e8IVz7vl8z33z5vx4388553NYxkJe\n",
       "63l9w2WMbIe9GCP569UYyWGvxhjyHOaVQRFNsorl/Jyv9m39jzML8R+WkhHxIyIGIK8MimiSH/BA\n",
       "X9d/Aq/yIjf29TsiIkbZ68BjfIzz+GenxbKbPWxQL8NURAyN6cAhEz57E7o/mPwB1/028H/go334\n",
       "noiIUdZ+rL2aOQD8gsM87q3tzRrf42Vbg44hot/eewy5XYcd9oCtuw/fExExyroeayuNL7wiRsJu\n",
       "3pr0aondvDWgaCIiok9SeEU0gHc5N7tHRIyAFF4RB7tOvWkTl0dERG8Kr1xkOIkJJB0pab2kv0r6\n",
       "raSZ+2k7TdJGSb+pM8amK8mhpHmSHpP0F0lPSbpyELE2jaTFksYlbZX04yna/KxavlnSad3W6V1+\n",
       "0ePeOuXfQdbb1i2Hkr5V5W6LpD9JOmUQcTZVyTZYtTtT0l5JX6kzvmFQuB+PVeePpyT9oeYQG61g\n",
       "Hz5K0jpJm6r8fWcAYU7S8Vjb4fiawmuylcB6258AflfNT+Uq4Gn69YqX4VWSwz3AD22fDJwNXCHp\n",
       "pBpjbBxJ04DbgcXAJ4ELJ+ZE0hLgRNvzgcuAO2sPtMFKcgg8B3zG9inAT2DymwJGVWH+3m13K7CO\n",
       "1ltAo1K4H88E7gC+aPtTwLLaA22owm1wBbDR9qm0BlS9TdLQXMFL4TXZUuC+avo+4MudGkmaCywB\n",
       "7iYHnom65tD2DtubqunXgGeA2bVF2ExnAdtsv2B7D/Ag8KUJbd7Lre0ngZmSjq43zEbrmkPbT9j+\n",
       "dzX7JDC35hibrGQbBPg+8BB0HqdoxJXk8JvAw7a3A9h+ueYYm6wkf/8AZlTTM4BXbO+tMcaepPCa\n",
       "7GjbO6vpncBUJ7WfAtfQGhkp9lWaQwAkHQecRuskOMrmAC+1zW+vPuvWJoXD+0py2O5SYG1fIxou\n",
       "XfMnaQ6tE+G7va3p8d9XyTY4Hziyut1ig6Rv1xZd85XkbzVwsqS/A5tpXX0aGkPTNXcgSVoPzOqw\n",
       "6Pr2GdvuNICrpAuAXbY3Hgwv7Pwwes1h23oOp/XL+aqq52uUlZ7AJvaw5sT3vuJcSDoPuARY1L9w\n",
       "hk5J/lYBK6t9W6THf6KSHE4HTgc+CxwKPCHpz3bG06Msf9cBm2yPSfo4sF7SQtv/7XNsB8RIFl62\n",
       "Pz/VMkk7Jc2yvUPSMcCuDs3OAZZW99scAsyQdL/ti/oUcuMcgBwiaTrwMPCA7V/1KdRh8jdgXtv8\n",
       "PFq/9vbXZm71WbSU5JDqhvrVwGLbr9YU2zAoyd8ZwIOtmoujgC9I2mN7TT0hNl5JDl8CXrb9BvCG\n",
       "pD8CC4EUXmX5Owe4CcD2s5KeBxYAG2qJsEe51DjZGuDiavpiYFJBYPs62/NsHw98A/j9KBVdBbrm\n",
       "sPqlfA/wtO1VNcbWZBuA+ZKOk/QR4Ou0ctluDXARgKSzgX+1XdaNghxKOhZ4BFhue9sAYmyyrvmz\n",
       "fYLt46vj30PA91J07aNkP/41cG71ZPyhwKdpPagVZfkbBz4HUN3juoDWQzNDYSR7vLq4BfilpEuB\n",
       "F4CvAUiaDay2fX6H/8mlnn2V5HARsBzYImlj9X/X2l43gHgbwfZeSSuAR4FpwD22n5F0ebX8Lttr\n",
       "JS2RtA34H/DdAYbcOCU5BG4AjgDurHpt9tg+a1AxN0lh/mI/CvfjcUnrgC207hNebTuFF8Xb4M3A\n",
       "vZI20+pA+pHt3QML+gNq/EuyIyIiIg4WudQYERERUZMUXhERERE1SeEVERERUZMUXhERERE1SeEV\n",
       "ERERUZMUXhERERE1SeEVERERUZN3AMQDEtwtYmq6AAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10be30ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the word vectors you trained\n",
    "\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], bbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "\n",
    "Now, with the word vectors you trained, we are going to perform a simple sentiment analysis.\n",
    "\n",
    "For each sentence in the Stanford Sentiment Treebank dataset, we are going to use the average of all the word vectors in that sentence as its feature, and try to predict the sentiment level of the said sentence. The sentiment level of the phrases are represented as real values in the original dataset, here we'll just use five classes:\n",
    "\n",
    "    \"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"\n",
    "    \n",
    "which are represented by 0 to 4 in the code, respectively.\n",
    "\n",
    "For this part, you will learn to train a softmax regressor with SGD, and perform train/dev validation to improve generalization of your regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement some helper functions\n",
    "\n",
    "def getSentenceFeature(tokens, wordVectors, sentence):\n",
    "    \"\"\" Obtain the sentence feature for sentiment analysis by averaging its word vectors \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement computation for the sentence features given a         #\n",
    "    # sentence.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - wordVectors: word vectors for all tokens                    #\n",
    "    #   - sentence: a list of words in the sentence of interest       #\n",
    "    # Output:                                                         #\n",
    "    #   - sentVector: feature vector for the sentence                 #\n",
    "    ###################################################################\n",
    "    \n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    for word in sentence:\n",
    "        idx = tokens[word]\n",
    "        sentVector += wordVectors[idx]\n",
    "        \n",
    "    sentVector /= len(sentence)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return sentVector\n",
    "\n",
    "def softmaxRegression(features, labels, weights, regularization = 0.0, nopredictions = False):\n",
    "    \"\"\" Softmax Regression \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement softmax regression with weight regularization.        #\n",
    "    # Inputs:                                                         #\n",
    "    #   - features: feature vectors, each row is a feature vector     #\n",
    "    #   - labels: labels corresponding to the feature vectors         #\n",
    "    #   - weights: weights of the regressor                           #\n",
    "    #   - regularization: L2 regularization constant                  #\n",
    "    # Output:                                                         #\n",
    "    #   - cost: cost of the regressor                                 #\n",
    "    #   - grad: gradient of the regressor cost with respect to its    #\n",
    "    #           weights                                               #\n",
    "    #   - pred: label predictions of the regressor (you might find    #\n",
    "    #           np.argmax helpful)                                    #\n",
    "    ###################################################################\n",
    "    \n",
    "    prob = softmax(features.dot(weights))\n",
    "\n",
    "    if len(features.shape) > 1:\n",
    "        N = features.shape[0]\n",
    "    else:\n",
    "        N = 1\n",
    "        \n",
    "    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i)) + 1/2*|w|^2\n",
    "    cost = np.sum(-np.log(prob[range(N), labels])) / N \n",
    "    cost += 0.5 * regularization * np.sum(weights ** 2)\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE: compute the gradients and predictions\n",
    "#    print(prob)\n",
    "    pred = np.argmax(prob, axis=1)\n",
    "     \n",
    "#    print \"features shape %d,%d\" % features.shape\n",
    "#    print \"weights shape %d,%d\" % weights.shape\n",
    "#    print \"prob shape %d,%d\" % prob.shape\n",
    "#    print \"LABELS %s\" % labels \n",
    "#    print \" PREDS %s\" % pred\n",
    "\n",
    "\n",
    "    #for i in range(prob.shape[0]):\n",
    "    #    print str(prob[i]) + \" (row \" + str(i) + \") sum = \" + str(np.sum(prob[i]))\n",
    "\n",
    "    \n",
    "    # See http://cs224d.stanford.edu/lectures/CS224d-Lecture4.pdf\n",
    "    # for gradients need to calculate dE/d_theta. \n",
    "    # = dE/dy * dy/d_theta. \n",
    "    # = (y_hat - target) * (x)\n",
    "    #\n",
    "    # then include regularization, which is just adding the lambda * theta value (derivative of lambda * 1/2*theta^2)\n",
    "    #\n",
    "    # finally divide by the number of examples to match the cost function (1/m)\n",
    "    #\n",
    "    # the tricky part is vectorizing it so we get the values for each class and weight in one operation\n",
    "\n",
    "#    print \"y_hat:\\n%s\\n\\n\" % prob\n",
    "    \n",
    "    # cross entropy + softmax derivative\n",
    "    prob[range(N), labels] -= 1\n",
    "    \n",
    "#    print \"delta:\\n%s\\n\\n\" % prob\n",
    "\n",
    "    # derivative input (weights * features) WRT to weights, then summed over the examples\n",
    "    grad = features.T.dot(prob)\n",
    "    \n",
    "    # now include the derivative of the regularization\n",
    "\n",
    "#    print \"grad:\\n%s\\n\\n\" % grad\n",
    "    \n",
    "    grad = grad / N\n",
    "\n",
    "    grad += regularization * weights\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    if nopredictions:\n",
    "        return cost, grad\n",
    "    else:\n",
    "        return cost, grad, pred\n",
    "\n",
    "def precision(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "def softmax_wrapper(features, labels, weights, regularization = 0.0):\n",
    "    cost, grad, _ = softmaxRegression(features, labels, weights, regularization)\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for softmax regression ====\n",
      "Gradient check passed!\n",
      "\n",
      "=== For autograder ===\n",
      "(1.9087572203226311, array([[ 0.13471085,  0.0104175 ,  0.06464052,  0.14678234,  0.07468882],\n",
      "       [-0.11082732, -0.0753506 ,  0.00940976,  0.09072251,  0.02397455],\n",
      "       [ 0.02857999, -0.07050319,  0.00609608, -0.00242226, -0.08065193],\n",
      "       [ 0.1165844 ,  0.23443669, -0.03104372, -0.05643964, -0.03242766],\n",
      "       [ 0.16402121,  0.07514553, -0.06366103,  0.00533061, -0.24784793],\n",
      "       [ 0.00873329, -0.03116552,  0.28757243,  0.11892055,  0.09353462],\n",
      "       [-0.14699131, -0.16446357, -0.23492789, -0.14272223,  0.08556511],\n",
      "       [ 0.10900431,  0.06290933, -0.03857583,  0.10949341, -0.11392697],\n",
      "       [-0.25459777,  0.17492415,  0.07520622,  0.09168167,  0.02080977],\n",
      "       [-0.05482908, -0.0023394 ,  0.10969091, -0.06085632,  0.10433387]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "# Gradient check always comes first\n",
    "random.seed(314159)\n",
    "np.random.seed(265)\n",
    "dummy_weights = 0.1 * np.random.randn(dimVectors, 5)\n",
    "dummy_features = np.zeros((10, dimVectors))\n",
    "dummy_labels = np.zeros((10,), dtype=np.int32)    \n",
    "for i in xrange(10):\n",
    "    words, dummy_labels[i] = dataset.getRandomTrainSentence()\n",
    "    dummy_features[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "print \"==== Gradient check for softmax regression ====\"\n",
    "gradcheck_naive(lambda weights: softmaxRegression(dummy_features, dummy_labels, weights, 1, nopredictions = True), dummy_weights)\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "print softmaxRegression(dummy_features, dummy_labels, dummy_weights, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGULARIZATION IS 0.000000\n",
      "*** iteration 1000: cost = 1.567704\n",
      "*** iteration 2000: cost = 1.565386\n",
      "*** iteration 3000: cost = 1.563709\n",
      "*** iteration 4000: cost = 1.562423\n",
      "*** iteration 5000: cost = 1.561398\n",
      "*** iteration 6000: cost = 1.560558\n",
      "*** iteration 7000: cost = 1.559860\n",
      "*** iteration 8000: cost = 1.559271\n",
      "*** iteration 9000: cost = 1.558770\n",
      "*** iteration 10000: cost = 1.558341\n",
      "Dev precision (%): 30.699364 with regularization 0.000000\n",
      "updating best reg to 0.000000, (30.699364 > 0.000000)\n",
      "REGULARIZATION IS 0.000010\n",
      "*** iteration 1000: cost = 1.567956\n",
      "*** iteration 2000: cost = 1.565790\n",
      "*** iteration 3000: cost = 1.564312\n",
      "*** iteration 4000: cost = 1.563243\n",
      "*** iteration 5000: cost = 1.562440\n",
      "*** iteration 6000: cost = 1.561819\n",
      "*** iteration 7000: cost = 1.561332\n",
      "*** iteration 8000: cost = 1.560945\n",
      "*** iteration 9000: cost = 1.560635\n",
      "*** iteration 10000: cost = 1.560385\n",
      "Dev precision (%): 30.699364 with regularization 0.000010\n",
      "REGULARIZATION IS 0.000030\n",
      "*** iteration 1000: cost = 1.568392\n",
      "*** iteration 2000: cost = 1.566449\n",
      "*** iteration 3000: cost = 1.565264\n",
      "*** iteration 4000: cost = 1.564496\n",
      "*** iteration 5000: cost = 1.563980\n",
      "*** iteration 6000: cost = 1.563624\n",
      "*** iteration 7000: cost = 1.563374\n",
      "*** iteration 8000: cost = 1.563196\n",
      "*** iteration 9000: cost = 1.563069\n",
      "*** iteration 10000: cost = 1.562977\n",
      "Dev precision (%): 30.881017 with regularization 0.000030\n",
      "updating best reg to 0.000030, (30.881017 > 30.699364)\n",
      "REGULARIZATION IS 0.000100\n",
      "*** iteration 1000: cost = 1.569374\n",
      "*** iteration 2000: cost = 1.567784\n",
      "*** iteration 3000: cost = 1.567107\n",
      "*** iteration 4000: cost = 1.566802\n",
      "*** iteration 5000: cost = 1.566661\n",
      "*** iteration 6000: cost = 1.566594\n",
      "*** iteration 7000: cost = 1.566562\n",
      "*** iteration 8000: cost = 1.566546\n",
      "*** iteration 9000: cost = 1.566538\n",
      "*** iteration 10000: cost = 1.566534\n",
      "Dev precision (%): 30.336058 with regularization 0.000100\n",
      "REGULARIZATION IS 0.000300\n",
      "*** iteration 1000: cost = 1.570054\n",
      "*** iteration 2000: cost = 1.568999\n",
      "*** iteration 3000: cost = 1.568849\n",
      "*** iteration 4000: cost = 1.568827\n",
      "*** iteration 5000: cost = 1.568824\n",
      "*** iteration 6000: cost = 1.568824\n",
      "*** iteration 7000: cost = 1.568824\n",
      "*** iteration 8000: cost = 1.568824\n",
      "*** iteration 9000: cost = 1.568824\n",
      "*** iteration 10000: cost = 1.568824\n",
      "Dev precision (%): 28.701181 with regularization 0.000300\n",
      "REGULARIZATION IS 0.001000\n",
      "*** iteration 1000: cost = 1.570192\n",
      "*** iteration 2000: cost = 1.570142\n",
      "*** iteration 3000: cost = 1.570142\n",
      "*** iteration 4000: cost = 1.570142\n",
      "*** iteration 5000: cost = 1.570142\n",
      "*** iteration 6000: cost = 1.570142\n",
      "*** iteration 7000: cost = 1.570142\n",
      "*** iteration 8000: cost = 1.570142\n",
      "*** iteration 9000: cost = 1.570142\n",
      "*** iteration 10000: cost = 1.570142\n",
      "Dev precision (%): 25.340599 with regularization 0.001000\n",
      "REGULARIZATION IS 0.003000\n",
      "*** iteration 1000: cost = 1.570993\n",
      "*** iteration 2000: cost = 1.570993\n",
      "*** iteration 3000: cost = 1.570993\n",
      "*** iteration 4000: cost = 1.570993\n",
      "*** iteration 5000: cost = 1.570993\n",
      "*** iteration 6000: cost = 1.570993\n",
      "*** iteration 7000: cost = 1.570993\n",
      "*** iteration 8000: cost = 1.570993\n",
      "*** iteration 9000: cost = 1.570993\n",
      "*** iteration 10000: cost = 1.570993\n",
      "Dev precision (%): 25.522252 with regularization 0.003000\n",
      "REGULARIZATION IS 0.010000\n",
      "*** iteration 1000: cost = 1.572645\n",
      "*** iteration 2000: cost = 1.572645\n",
      "*** iteration 3000: cost = 1.572645\n",
      "*** iteration 4000: cost = 1.572645\n",
      "*** iteration 5000: cost = 1.572645\n",
      "*** iteration 6000: cost = 1.572645\n",
      "*** iteration 7000: cost = 1.572645\n",
      "*** iteration 8000: cost = 1.572645\n",
      "*** iteration 9000: cost = 1.572645\n",
      "*** iteration 10000: cost = 1.572645\n",
      "Dev precision (%): 25.522252 with regularization 0.010000\n",
      "Best is 30.881017 with regularization 0.000030\n"
     ]
    }
   ],
   "source": [
    "# Try different regularizations and pick the best!\n",
    "\n",
    "### YOUR CODE HERE\n",
    "regularizations = [0.0, 0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01]\n",
    "\n",
    "best = 0\n",
    "bestReg = 0\n",
    "precisions = []\n",
    "bestWeights = None\n",
    "for regularization in regularizations:\n",
    "    print 'REGULARIZATION IS %f' % regularization\n",
    "#regularization = 0.0 # try 0.0, 0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01 and pick the best\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "    random.seed(3141)\n",
    "    np.random.seed(59265)\n",
    "    weights = np.random.randn(dimVectors, 5)\n",
    "\n",
    "    trainset = dataset.getTrainSentences()\n",
    "    nTrain = len(trainset)\n",
    "    trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "\n",
    "    for i in xrange(nTrain):\n",
    "        words, trainLabels[i] = trainset[i]\n",
    "        trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    \n",
    "    # We will do batch optimization\n",
    "    weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, weights, regularization), weights, 3.0, 10000, PRINT_EVERY=1000)\n",
    "\n",
    "    # Prepare dev set features\n",
    "    devset = dataset.getDevSentences()\n",
    "    nDev = len(devset)\n",
    "    devFeatures = np.zeros((nDev, dimVectors))\n",
    "    devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "\n",
    "    for i in xrange(nDev):\n",
    "        words, devLabels[i] = devset[i]\n",
    "        devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    \n",
    "    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "    \n",
    "    prec = precision(devLabels, pred)\n",
    "    print \"Dev precision (%%): %f with regularization %f\" % (prec, regularization)\n",
    "    precisions.append(prec)\n",
    "    if prec > best:\n",
    "        print 'updating best reg to %f, (%f > %f)' % (regularization, prec, best)\n",
    "        best = prec\n",
    "        bestReg = regularization\n",
    "        bestWeights = weights\n",
    "    \n",
    "print 'Best is %f with regularization %f' % (best, bestReg)\n",
    "weights = bestWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "1\t0\n"
     ]
    }
   ],
   "source": [
    "# Write down the best regularization and accuracy you found\n",
    "# sanity check: your accuracy should be around or above 30%\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "BEST_REGULARIZATION = 1\n",
    "BEST_ACCURACY = 0.0\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "print \"=== For autograder ===\\n%g\\t%g\" % (BEST_REGULARIZATION, BEST_ACCURACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Test precision (%): 28.235294\n"
     ]
    }
   ],
   "source": [
    "# Test your findings on the test set\n",
    "\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "\n",
    "for i in xrange(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    \n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, weights)\n",
    "print \"=== For autograder ===\\nTest precision (%%): %f\" % precision(testLabels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Credit\n",
    "\n",
    "Train your own classifier for sentiment analysis! We will not provide any starter code for this part, but you can feel free to reuse the code you've written before, or write some new code for this task. Also feel free to refer to the code we provided you with to see how we scaffolded training for you.\n",
    "\n",
    "Try to contain all of your code in one code block. You could start by using multiple blocks, then paste code together and remove unnecessary blocks. Report, as the last two lines of the output of your block, the dev set accuracy and test set accuracy you achieved, in the format we used above.\n",
    "\n",
    "*Note: no credits will be given for this part if you use the dev or test sets for training, or if you fine-tune your regularization or other hyperparameters on the test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "### END YOU CODE\n",
    "\n",
    "\n",
    "_, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "print \"=== For autograder ===\\nDev precision (%%): %f\" % precision(devLabels, pred)\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, weights)\n",
    "print \"Test precision (%%): %f\" % precision(testLabels, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
