\documentclass[]{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}


%\newcommand{\deriv}[3][]{% \deriv[<order>]{<func>}{<var>}
%	\ensuremath{\frac{\partial^{#1} {#2}}{\partial {#3}^{#1}}}}

\newcommand{\derivop}[1]{% \deriv{<func>}{<var>}
	\ensuremath{\frac{\partial}{\partial {#1}}}}

\newcommand{\deriv}[2]{% \deriv{<func>}{<var>}
	\ensuremath{\frac{\partial {#1}}{\partial {#2}}}}

\newcommand{\slayer}[3]{ % scalar element of layer variable/matrix
	\ensuremath{{#1}^{(#2)}_{#3}}}

\newcommand{\vlayer}[2]{ % vector/matrix layer variable
	\ensuremath{{#1}^{(#2)}}}

\newcommand{\reals}[1]{ % vector/matrix layer variable
	\ensuremath{\in \mathbb{R}^{#1}}}

%opening
\title{Problem Set 2, Problem 3}
\author{Kireet}

\begin{document}

\maketitle

\begin{abstract}
This doc contains the derivations needed to complete the coding of the recursive neural net problem. See Lecture Notes 4 for background.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
This document will discuss the Recurrent Neural Network (RNN) problem. RNNs are similar to feed forward nets, but they also take in as input the hidden layer values from the previous "time step":
 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{rnn-timestep}
	\caption{A snapshot of the RNN hidden layer transformation at time step t}
	\label{fig:neural-net}
\end{figure}

In this problem, we are trying to build an RNN to do word prediction, each time step could be occurrence of a particular word. The RNN would output the prediction for the next word given the current input word and the accumulated context within the hidden layer.

\section{Setup}
Similar to the examples from the lecture notes, this problem deals with a single hidden layer RNN. The cost function is again the cross entropy loss function, however now we indicate the time step t at which the error occurred:

$$\vlayer{J}{t}(\theta) = - \sum_{j=1}^{|V|} \slayer{y}{t}{j} log(\slayer{\hat{y}}{t}{j})$$

The errors are summed across all time steps in the sequence to calculate the total loss for that sequence and then the errors for all sequences are summed to calculate the total loss for the data set.

The hidden layer at time step t is calculated as 
$$ \vlayer{h}{t} = \sigma\left(H\vlayer{h}{t-1} + L\vlayer{x}{t}\right)$$

where $\sigma$ is the sigmoid function. The output layer at time step t is calculated as 
$$ \hat{y} = \rho \left(U\vlayer{h}{t}\right)$$
where $\rho$ is the softmax function. The $\hat{y}$ vector is the probabilities the next word, i.e.:
$$P(x_{t+1} = v_j | x_t,\dots,x_1) = \slayer{\hat{y}}{t}{j}$$
$\vlayer{h}{0}$ is a fixed initialization vector. Here, $\vlayer{x}{t}$ is a one hot vector, and thus just "selects" a row from $L$. Thus our dimensions are:

$$H \reals{D_h \times D_h}, L \reals{D_h\times|V|}, U \reals{|V| \times D_h}$$

We can see $L$ contains the representations of the input word, $H$ is the hidden to hidden layer weights matrix, and $U$ is the output weight matrix. $|V|$ is the size of the vocabulary and $D_h$ is the size of the hidden layer.
\section{Understanding Back Propagation Through Time (BPTT)}
Some formulas were presented in the lectures without much justification. We'll go through them here. First we define the overall cost function for a sequence:
$$J = -\frac{1}{T}\sum_{t=1}^{T}\vlayer{J}{t}(\theta) = -\frac{1}{T}\sum_{t=1}^{T}\sum_{j=1}^{|V|}\slayer{y}{t}{j} log(\slayer{y}{t}{j})$$
This is simply the sum of all the costs at each time step. if we want to calculate $\deriv{J}{H}$, we can phrase this as
$$\deriv{J}{H} = \sum_{t=1}^{T} \deriv{\vlayer{J}{t}}{H}$$
Remember H is the same at each time step. So calculating $\deriv{\vlayer{J}{t}}{H}$ is not straight forward. Let's take a look at this network "unrolled" through a few time steps:

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{rnn-unrolled}
	\caption{A RNN across 4 time steps}
	\label{fig:rnn-unrolled}
\end{figure}

Here we see an RNN through 4 time steps. Each time step has an input\vlayer{x}{t}, the current state of the hidden layer\vlayer{h}{t}, and an output vector\vlayer{y}{t} (really \vlayer{\hat{y}}{t}, but here it's y due to limited drawing tool).\vlayer{h}{0} is the initialization vector and each subsequent \vlayer{h}{t} depends on \vlayer{h}{t-1} via the weights matrix $H$. Note $H$ is the \textit{same} weights matrix at each time step!
\paragraph{}
\vlayer{J}{3} here is a function of\vlayer{\hat{y}}{3}. But\vlayer{\hat{y}}{3}is a function of all the previous hidden layers and $H$. The thing to do is think of things element-wise. Let's take a particular weight, $H_{12}$ (highlighted as the red arrow). For clarity, the diagram has labeled each instance of $H_{12}$ separately as \slayer{H}{t}{12}, but again remember it's the same weight at each time step. So to compute how $J$ changes with $H_{12}$, we need to sum how it changes with each particular "instance" of $H_{12}$ in the network:

$$\deriv{\vlayer{J}{3}}{H_{12}} = \sum_{i=0}^{t-1} \deriv{\vlayer{J}{3}}{\slayer{H}{i}{12}}$$
$$ = \deriv{\vlayer{J}{3}}{\vlayer{h}{3}}\cdot\deriv{\vlayer{h}{3}}{H_{12}} + \deriv{\vlayer{J}{3}}{\vlayer{h}{2}}\cdot\deriv{\vlayer{h}{2}}{H_{12}} + \deriv{\vlayer{J}{3}}{\vlayer{h}{1}}\cdot\deriv{\vlayer{h}{1}}{H_{12}}$$

The important thing to see here is the first element of the sum is only considering the impact of \slayer{H}{2}{12},\vlayer{h}{2} is considered as fixed in that element. Similarly, the second term only considers \slayer{H}{1}{12} and the final term only\slayer{H}{0}{12}. Now we can generalize the above formula for $t$ time steps and with respect to each weight $H_{ij}$ represented by the full matrix $H$:

$$\deriv{\vlayer{J}{3}}{H} = \sum_{i=1}^{t} \deriv{\vlayer{J}{t}}{\vlayer{h}{i}}\cdot\deriv{\vlayer{h}{i}}{H}$$
Applying the chain rule:
$$\deriv{\vlayer{J}{3}}{H} = \sum_{i=1}^{t} \deriv{\vlayer{J}{t}}{\vlayer{\hat{y}}{t}}\cdot\deriv{\vlayer{\hat{y}}{t}}{\vlayer{h}{t}}\cdot\deriv{\vlayer{h}{t}}{\vlayer{h}{i}}\cdot\deriv{\vlayer{h}{i}}{H}$$

The difficult thing here to calculate is $\deriv{\vlayer{h}{t}}{\vlayer{h}{i}}$. Via the chain rule we can see that this is simply:

$$\prod_{k=i+1}^{t}\deriv{\vlayer{h}{k}}{\vlayer{h}{k-1}}$$

To calculate $\deriv{\vlayer{h}{k}}{\vlayer{h}{k-1}}$, we can perform some element wise calculations. Note \vlayer{x}{t} as a subscript just means the index of the one hot element:

$$\deriv{\slayer{h}{t}{i}}{\slayer{h}{t-1}{j}} = \deriv{}{\slayer{h}{t-1}{j}}\sigma\left(H\vlayer{h}{t-1} + L\vlayer{x}{t}\right)_{i}$$
$$ = \deriv{}{\slayer{h}{t-1}{j}}\sigma\left(\sum_kH_{ik}\slayer{h}{t-1}{k} + L_{x^{(t)}k}\right)$$
$$ = \sigma'\left(\sum_kH_{ik}\slayer{h}{t-1}{k} + L_{ik}\right) \cdot H_{ij}$$

Let's define the $\sigma'(...)$ term as $\sigma'_i$ for notational convenience. Then in Jacobian matrix form:
\[
\deriv{\vlayer{h}{k}}{\vlayer{h}{k-1}} = 
\begin{bmatrix}
\deriv{\slayer{h}{k}{1}}{\slayer{h}{k-1}{1}}	&	\deriv{\slayer{h}{k}{1}}{\slayer{h}{k-1}{2}}        & \dots & \deriv{\slayer{h}{k}{1}}{\slayer{h}{k-1}{D_h}} \\
\vdots	&	\vdots	&	\ddots	& \vdots \\
\deriv{\slayer{h}{k}{D_h}}{\slayer{h}{k-1}{1}}	&	\deriv{\slayer{h}{k}{D_h}}{\slayer{h}{k-1}{2}}	& \dots & \deriv{\slayer{h}{k}{D_h}}{\slayer{h}{k-1}{D_h}} \\
\end{bmatrix}
\]
\[
\deriv{\vlayer{h}{k}}{\vlayer{h}{k-1}} = 
\begin{bmatrix}
\sigma'_1H_{11}       & \sigma'_1H_{12} & \sigma'_1H_{13} & \dots & \sigma'_1H_{1D_h} \\
\sigma'_2H_{21}       & \sigma'_2H_{22} & \sigma'_2H_{23} & \dots & \sigma'_2H_{2D_h} \\
\vdots	& \vdots	&\vdots	&\ddots	& \vdots \\
\sigma'_{D_h}H_{D_h1}       & \sigma'_{D_h}H_{D_h2} & \sigma'_{D_h}H_{D_h3} & \dots & \sigma'_{D_h}H_{D_hD_h} \\
\end{bmatrix}
\]
$$= diag(\sigma') \cdot H$$

The diag as the leading matrix in a multiplication simply weights the 2nd matrix's rows by the diagonal elements of the first. In later derivations, it will often be useful to phrase this derivative as the inverse Jacobian. In that case we have:

$$\deriv{\vlayer{h}{k}}{\vlayer{h}{k-1}} = H^\top \cdot diag(\sigma')$$

\section{Part a}
Part (a) asks us to show that minimizing the cross entropy loss function from Section 2 will minimize perplexity, given as :
$$ PP(y, \hat{y}) = \frac{1}{\sum_{j=1}^{|V|}\slayer{y}{t}{j}\cdot\slayer{\hat{y}}{t}{j}}$$

Given a correct class k, we have:
$$J(\theta) = -log(\slayer{\hat{y}}{t}{k}) , PP(y, \hat{y}) = \frac{1}{\slayer{\hat{y}}{t}{k}} $$
It's clear that as $\hat{y}$ increases, both values monotonically decrease. Thus minimizing the cross entropy loss should minimize perplexity. 

\section{Part b}
Part b asks us to calculate the gradients a time step t, with the previous time steps considered fixed. As before, we'll proceed element wise and then convert the results to vector notation/operations. We'll again change some notation. $U$ and $H$ will be unchanged but we'll again use $a$ and $z$ to refer to layer outputs and inputs, respectively.\vlayer{a}{h,t} and\vlayer{z}{h,t} will refer to the output and input of the hidden layer at time $t$. Similarly, $(x,t)$ and $(o,t)$ superscript will refer to the input and output layer at time $t$.

\subsection{Output Layer (U) Gradients}
First we calculate $\deriv{\vlayer{J}{t}}{U}$. As we've seen before, the derivative of the softmax and cross entropy loss combination simply results in a $\hat{y} - y$ vector. We term the input to the final output layer at time t as\vlayer{z}{o,t}
$$\deriv{\vlayer{J}{t}}{U} = \deriv{\vlayer{J}{t}}{\vlayer{z}{o,t}}\cdot\deriv{\vlayer{z}{o,t}}{U} \triangleq\vlayer{\delta}{o,t}\cdot\deriv{\vlayer{z}{o,t}}{U}$$
Element wise for the 2nd term:
$$ \deriv{\slayer{z}{o,t}{i}}{U_{ij}} = \deriv{}{U_{ij}} \sum_{k} U_{ik}\slayer{h}{t}{k}$$
$$ = \slayer{h}{t}{j}$$

In matrix form we have:
\[
\nabla_{U} = 
\begin{bmatrix}
\slayer{\delta}{o,t}{1}\slayer{h}{t}{1}       & \slayer{\delta}{o,t}{1}\slayer{h}{t}{2} & \slayer{\delta}{o,t}{1}\slayer{h}{t}{3} & \dots & \slayer{\delta}{o,t}{1}\slayer{h}{t}{D_h} \\
\slayer{\delta}{o,t}{2}\slayer{h}{t}{1}       & \slayer{\delta}{o,t}{2}\slayer{h}{t}{2} & \slayer{\delta}{o,t}{2}\slayer{h}{t}{3} & \dots & \slayer{\delta}{o,t}{2}\slayer{h}{t}{D_h} \\
\vdots	& \vdots	&\vdots	&\ddots	& \vdots \\
\slayer{\delta}{o,t}{|V|}\slayer{h}{t}{1}       & \slayer{\delta}{o,t}{|V|}\slayer{h}{t}{2} & \slayer{\delta}{o,t}{|V|}\slayer{h}{t}{3} & \dots & \slayer{\delta}{o,t}{|V|}\slayer{h}{t}{D_h} \\
\end{bmatrix}
\]
Thus:
\begin{equation} \label{eq:d3,t}
\nabla_U = (\vlayer{\delta}{3,t})(\vlayer{h}{t})^\top
\end{equation}We can check the dimensions and see we end up with $\nabla_U \reals{|V| \times D_h}$, which is correct as it matches the dimensions of $U$.

\subsection{Hidden Layer Weights (H) Gradients}\label{subsection_Ht}
Next we calculate \deriv{\vlayer{J}{t}}{H}, again element-wise. We define $\vlayer{z}{h,t}$ as the input to the hidden layer:
$$\deriv{\vlayer{J}{t}}{H_{ij}} = \deriv{\vlayer{J}{t}}{\slayer{z}{h,t}{i}} \cdot \deriv{\slayer{z}{h,t}{i}}{H_{ij}}$$

First we derive the first term:
$$\deriv{\vlayer{J}{t}}{\slayer{z}{h,t}{i}} = \deriv{\vlayer{J}{t}}{\slayer{a}{h,t}{i}}\cdot\deriv{\slayer{a}{h,t}{i}}{\slayer{z}{h,t}{i}}$$
$$= \deriv{\vlayer{J}{t}}{\slayer{a}{h,t}{i}} \cdot \sigma'(\slayer{z}{h,t}{i})$$
$$= \sigma'(\slayer{z}{h,t}{i})\cdot \left(\deriv{\vlayer{J}{t}}{\slayer{z}{o,t}{i}}\cdot\deriv{\slayer{z}{o,t}{i}}{\slayer{a}{h,t}{i}}\right)$$
$$ = \sigma'(\slayer{z}{h,t}{i})\cdot \sum_k \left[\slayer{\delta}{o,t}{k} \deriv{}{\slayer{a}{h,t}{i}}\left(\ \sum_l U_{kl} \slayer{a}{h,t}{l} \right)\right]$$
$$ \slayer{\delta}{h,t}{i} \triangleq \sigma'(\slayer{z}{h,t}{i}) \cdot \sum_k \slayer{\delta}{o,t}{k} U_{ki}$$
The second term derivation:
$$\deriv{\slayer{z}{h,t}{i}}{H_{ij}} = \deriv{}{H_{ij}}\left(\sum_k H_{ik}\slayer{a}{h,t-1}{k} + \sum_l L_{il}\slayer{x}{t}{l}\right)$$
$$= \slayer{a}{h,t-1}{j}$$

Combining these, we have:
$$\deriv{\vlayer{J}{t}}{H_{ij}} = \slayer{\delta}{h,t}{i} \cdot \slayer{a}{h,t-i}{j}$$

Similar to the layer 1 weights in the previous feed forward network problem, the vectorization of this is:

\begin{equation} \label{eq:dh,t}
\vlayer{\delta}{h,t} = \sigma'(\vlayer{z}{h,t}) \circ (U^\top \vlayer{\delta}{o,t})
\end{equation}

\begin{equation} \label{eq:H,t}
\nabla_H = \vlayer{\delta}{h,t} (\vlayer{a}{h,t-1})^\top)
\end{equation}

We can check the dimensions, $U^\top$\reals{D_h\times|V|} and\vlayer{delta}{o,t}\reals{|V|} means that their product\reals{D_h}. This is correct, we should have an error term for each hidden layer unit. For equation \ref{eq:H,t}, we see that \vlayer{\delta}{h,t}\reals{D_h} from equation \ref{eq:dh,t} and $\vlayer{a}{h,t-1})^\top$\reals{1 \times D_h} which means $\nabla_H$\reals{D_h \times D_h}. This matches the dimensions of $H$.

\subsection{Input ($L_{\vlayer{x}{t}}$) Gradients}
\vlayer{x}{t} is simply a one hot vector that identifies the input word at time $t$. Thus multiplying this vector by $L$ simply extracts a word vector $L_{\vlayer{x}{t}}$\reals{D_h}. Unlike the feed forward network, we can see that there is no weights matrix used to combine this word vector into each hidden unit. Instead the $i^{th}$ hidden unit's sigmoid function simply receives the $i^{th}$ value from the word vector. Now we start our element wise derivation, using this fact:
$$\deriv{\vlayer{J}{t}}{L_{\vlayer{x}{t}i}} = \sum_k \deriv{\vlayer{J}{t}}{\slayer{z}{t}{k}} \cdot \deriv{\slayer{z}{t}{k}}{L_{\vlayer{x}{t}i}}$$
$$ = \deriv{\vlayer{J}{t}}{\slayer{z}{t}{i}} \cdot \deriv{\slayer{z}{t}{i}}{L_{\vlayer{x}{t}i}}$$
$$ = \slayer{\delta}{h,t}{i} \cdot \deriv{}{L_{\vlayer{x}{t}i}}\left( \sum_k H_{ik}\slayer{a}{h,t-1}{k} + L_{\vlayer{x}{t}i} \right)$$
$$ = \slayer{\delta}{h,t}{i}$$

The vectorization is simply:
\begin{equation} \label{eq:L,t}
\nabla_{L_{\vlayer{x}{t}}} = \vlayer{\delta}{h,t}
\end{equation}

\subsection{Previous Time Step ($\vlayer{h}{t-1}$) Gradients}
Finally we are asked to compute the gradients with respect to the previous time step. In our notation, this is\vlayer{a}{h,t-1}.
$$\deriv{\vlayer{J}{t}}{\slayer{a}{h, t-1 }{j}} = \sum_i \deriv{\vlayer{J}{t}}{\slayer{z}{h,t}{i}} \cdot \deriv{\slayer{z}{h,t}{i}}{\slayer{a}{h,t-1}{j}}$$
$$ = \sum_i \slayer{\delta}{h,t}{i} \cdot \deriv{}{\slayer{a}{h,t-1}{j}}\left( \sum_k H_{ik}\slayer{a}{h,t-1}{k} + L_{\vlayer{x}{t}i} \right)$$
$$ = \sum_i \slayer{\delta}{h,t}{i} \cdot H_{ij}$$
Before vectorizing, let's examine this formula. We see that for the error gradient at a hidden layer node at time $t-1$ is simply a weighted sum of the errors at time step $t$, with the weight being the appropriate value in the weights matrix. That is the weight from the $t-1$ node to all the nodes at time $t$. This is essentially the same logic as the feed forward network derivations. 

Now in matrix form:
\[
\nabla_{\vlayer{a}{h,t-1}} = 
\begin{bmatrix}
\slayer{\delta}{h,t}{1}H_{11} + \slayer{\delta}{h,t}{2}H_{21} + \slayer{\delta}{h,t}{3}H_{31} + \dots + \slayer{\delta}{h,t}{D_h}H_{D_h1} \\
\slayer{\delta}{h,t}{1}H_{12} + \slayer{\delta}{h,t}{2}H_{22} + \slayer{\delta}{h,t}{3}H_{32} + \dots + \slayer{\delta}{h,t}{D_h}H_{D_h2} \\
\vdots \\
\slayer{\delta}{h,t}{1}H_{1D_h} + \slayer{\delta}{h,t}{2}H_{2D_h} + \slayer{\delta}{h,t}{3}H_{3D_h} + \dots + \slayer{\delta}{h,t}{D_h}H_{D_hD_h} \\

\end{bmatrix}
\]

\begin{equation} \label{eq:h,t-1}
\nabla_{\vlayer{a}{h,t-1}} = H^\top \vlayer{\delta}{h,t}
\end{equation}

\section{Part c}
Part c asks us to calculate the $t-1$ derivatives for $L_{\vlayer{x}{t-1}}$ and $H$.

\subsection{Input ($L_{\vlayer{x}{t-1}}$) Gradients}
To calculate $\deriv{\vlayer{J}{t}}{L_{\vlayer{x}{t-1}}}$, we can use the results from the previous part:

$$\deriv{\vlayer{J}{t}}{L_{\vlayer{x}{t-1}}} = \deriv{\vlayer{J}{t}}{\vlayer{a}{h,t-1}}\cdot\deriv{\vlayer{a}{h, t-1}}{\vlayer{z}{h,t-1}}\cdot\deriv{\vlayer{z}{h, t-1}}{L_{\vlayer{x}{t-1}}}$$
$$= \deriv{\vlayer{J}{t}}{\vlayer{a}{h,t-1}}\cdot\deriv{\vlayer{a}{h, t-1}}{\vlayer{z}{h,t-1}}\cdot \deriv{}{L_{\vlayer{x}{t-1}}} \left(H\vlayer{a}{t-1} + L_{\vlayer{x}{t-1}}\right)$$

\begin{equation} \label{eq:h,t-1}
\nabla_{L_{\vlayer{x}{h,t-1}}}= \sigma'(\vlayer{z}{h,t-1}) \circ H^\top \vlayer{\delta}{h,t}\end{equation}

Note this is also $\deriv{\vlayer{J}{t}}{\vlayer{z}{h,t-1}}$ and thus is our \vlayer{\delta}{h,t-1}:

\begin{equation} \label{eq:delta,t-1}
\vlayer{\delta}{h,t-1}= \sigma'(\vlayer{z}{h,t-1}) \circ H^\top \vlayer{\delta}{h,t}\end{equation}

\subsection{Hidden Layer Weights ($H$) Gradients}
We again attempt to use previously calculated values to calculate the hidden layer weights matrix gradients at time step $t-1$:

$$\left. \deriv{\vlayer{J}{t}}{H}\right|_{t-1} = \deriv{\vlayer{J}{t}}{\vlayer{z}{h,t-1}}\cdot\deriv{\vlayer{z}{h,t-1}}{H}$$

The first term we know to be\vlayer{\delta}{h,t-1}. The second term derivation is essentially identical to the latter steps of subsection \ref{subsection_Ht}, resulting in $(\vlayer{a}{h,t-2})^\top$. Thus we have:
\begin{equation}
\left.\nabla_H\right|_{t-1} = \vlayer{\delta}{h,t-1} \cdot (\vlayer{a}{h,t-2})^\top
\end{equation}
Using these equation, we can backprop arbitrarily back in time by continually calculating the $\delta$ error values at each time step.
\end{document}