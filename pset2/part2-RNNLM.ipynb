{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [2]: Recurrent Neural Networks\n",
    "\n",
    "This notebook will provide starter code, testing snippets, and additional guidance for implementing the Recurrent Neural Network Language Model (RNNLM) described in Part 2 of the handout.\n",
    "\n",
    "Please complete parts (a), (b), and (c) of Part 2 before beginning this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Implement a Recurrent Neural Network Language Model\n",
    "\n",
    "Follow the instructions on the handout to implement your model in `rnnlm.py`, then use the code below to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: temporarily setting self.bptt = len(y) = 3 to compute true gradient.\n",
      "grad_check: dJ/dH error norm = 6.67e-10 [ok]\n",
      "    H dims: [50, 50] = 2500 elem\n",
      "grad_check: dJ/dU error norm = 1.014e-09 [ok]\n",
      "    U dims: [10, 50] = 500 elem\n",
      "grad_check: dJ/dL[3] error norm = 4.208e-10 [ok]\n",
      "    L[3] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[2] error norm = 4.199e-10 [ok]\n",
      "    L[2] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[1] error norm = 4.192e-10 [ok]\n",
      "    L[1] dims: [50] = 50 elem\n",
      "Reset self.bptt = 20\n"
     ]
    }
   ],
   "source": [
    "from rnnlm import RNNLM\n",
    "# Gradient check on toy data, for speed\n",
    "random.seed(10)\n",
    "wv_dummy = random.randn(10,50)\n",
    "model = RNNLM(L0 = wv_dummy, U0 = wv_dummy,\n",
    "              alpha=0.005, rseed=10, bptt=20)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vocabulary and Load PTB Data\n",
    "\n",
    "We've pre-prepared a list of the vocabulary in the Penn Treebank, along with their absolute counts and unigram frequencies. The document loader code below will \"canonicalize\" words and replace any unknowns with a `\"UUUNKKK\"` token, then convert the data to lists of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 38444 (84.00% of all tokens)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import utils as du\n",
    "import pandas as pd\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab = pd.read_table(\"data/lm/vocab.ptb.txt\", header=None, sep=\"\\s+\",\n",
    "                     index_col=0, names=['count', 'freq'], )\n",
    "\n",
    "# Choose how many top words to keep\n",
    "vocabsize = 2000\n",
    "num_to_word = dict(enumerate(vocab.index[:vocabsize]))\n",
    "word_to_num = du.invert_dict(num_to_word)\n",
    "##\n",
    "# Below needed for 'adj_loss': DO NOT CHANGE\n",
    "fraction_lost = float(sum([vocab['count'][word] for word in vocab.index\n",
    "                           if (not word in word_to_num) \n",
    "                               and (not word == \"UUUNKKK\")]))\n",
    "fraction_lost /= sum([vocab['count'][word] for word in vocab.index\n",
    "                      if (not word == \"UUUNKKK\")])\n",
    "print \"Retained %d words from %d (%.02f%% of all tokens)\" % (vocabsize, len(vocab),\n",
    "                                                             100*(1-fraction_lost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, using the vocabulary in `word_to_num`. Our starter code handles this for you, and also generates lists of lists X and Y, corresponding to input words and target words*. \n",
    "\n",
    "*(Of course, the target words are just the input words, shifted by one position, but it can be cleaner and less error-prone to keep them separate.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock , traders say .\n",
      "[   4  147  169  250 1879    7 1224   64    7    1    3    7  456    1    3\n",
      " 1024  255   24  378  147    3    6   67    0  255  138    2    5]\n"
     ]
    }
   ],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset('data/lm/ptb-train.txt')\n",
    "S_train = du.docs_to_indices(docs, word_to_num)\n",
    "X_train, Y_train = du.seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/lm/ptb-dev.txt')\n",
    "S_dev = du.docs_to_indices(docs, word_to_num)\n",
    "X_dev, Y_dev = du.seqs_to_lmXY(S_dev)\n",
    "\n",
    "# Load the test set (final evaluation only)\n",
    "docs = du.load_dataset('data/lm/ptb-test.txt')\n",
    "S_test = du.docs_to_indices(docs, word_to_num)\n",
    "X_test, Y_test = du.seqs_to_lmXY(S_test)\n",
    "\n",
    "# Display some sample data\n",
    "print \" \".join(d[0] for d in docs[7])\n",
    "print S_test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Train and evaluate your model\n",
    "\n",
    "When you're able to pass the gradient check, let's run our model on some real language!\n",
    "\n",
    "You should randomly initialize the word vectors as Gaussian noise, i.e. $L_{ij} \\sim \\mathit{N}(0,0.1)$ and $U_{ij} \\sim \\mathit{N}(0,0.1)$; the function `random.randn` may be helpful here.\n",
    "\n",
    "As in Part 1, you should tune hyperparameters to get a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: temporarily setting self.bptt = len(y) = 3 to compute true gradient.\n",
      "grad_check: dJ/dH error norm = 2.279e-09 [ok]\n",
      "    H dims: [100, 100] = 10000 elem\n",
      "grad_check: dJ/dU error norm = 5.83e-09 [ok]\n",
      "    U dims: [2000, 100] = 200000 elem\n",
      "grad_check: dJ/dL[3] error norm = 5.518e-10 [ok]\n",
      "    L[3] dims: [100] = 100 elem\n",
      "grad_check: dJ/dL[2] error norm = 5.131e-10 [ok]\n",
      "    L[2] dims: [100] = 100 elem\n",
      "grad_check: dJ/dL[1] error norm = 7.321e-10 [ok]\n",
      "    L[1] dims: [100] = 100 elem\n",
      "Reset self.bptt = 3\n"
     ]
    }
   ],
   "source": [
    "hdim = 100 # dimension of hidden layer = dimension of word vectors\n",
    "random.seed(10)\n",
    "L0 = zeros((vocabsize, hdim)) # replace with random init, \n",
    "                              # or do in RNNLM.__init__()\n",
    "# test parameters; you probably want to change these\n",
    "model = RNNLM(L0, U0 = L0, alpha=0.05, rseed=10, bptt=3)\n",
    "\n",
    "# Gradient check is going to take a *long* time here\n",
    "# since it's quadratic-time in the number of parameters.\n",
    "# run at your own risk... (but do check this!)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 56522\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 9.00999\n",
      "  Seen 1000 in 382.69 s\n",
      "  Seen 2000 in 403.13 s\n",
      "  Seen 3000 in 424.72 s\n",
      "  Seen 4000 in 446.17 s\n",
      "  Seen 5000 in 466.95 s\n",
      "  Seen 6000 in 487.96 s\n",
      "  Seen 7000 in 509.72 s\n",
      "  Seen 8000 in 530.50 s\n",
      "  Seen 9000 in 551.26 s\n",
      "  Seen 10000 in 572.53 s\n",
      "  [10000]: mean loss 4.63868\n",
      "  Seen 11000 in 961.23 s\n",
      "  Seen 12000 in 983.33 s\n",
      "  Seen 13000 in 1004.64 s\n",
      "  Seen 14000 in 1025.86 s\n",
      "  Seen 15000 in 1046.55 s\n",
      "  Seen 16000 in 1066.84 s\n",
      "  Seen 17000 in 1087.87 s\n",
      "  Seen 18000 in 1108.80 s\n",
      "  Seen 19000 in 1128.74 s\n",
      "  Seen 20000 in 1148.47 s\n",
      "  [20000]: mean loss 4.38977\n",
      "  Seen 21000 in 1530.79 s\n",
      "  Seen 22000 in 1551.61 s\n",
      "  Seen 23000 in 1572.37 s\n",
      "  Seen 24000 in 1593.40 s\n",
      "  Seen 25000 in 1614.09 s\n",
      "  Seen 26000 in 1634.29 s\n",
      "  Seen 27000 in 1654.94 s\n",
      "  Seen 28000 in 1674.44 s\n",
      "  Seen 29000 in 1694.70 s\n",
      "  Seen 30000 in 1714.92 s\n",
      "  [30000]: mean loss 4.2913\n",
      "  Seen 31000 in 2094.58 s\n",
      "  Seen 32000 in 2115.44 s\n",
      "  Seen 33000 in 2135.32 s\n",
      "  Seen 34000 in 2155.40 s\n",
      "  Seen 35000 in 2175.87 s\n",
      "  Seen 36000 in 2196.46 s\n",
      "  Seen 37000 in 2216.09 s\n",
      "  Seen 38000 in 2235.89 s\n",
      "  Seen 39000 in 2257.09 s\n",
      "  Seen 40000 in 2277.46 s\n",
      "  [40000]: mean loss 4.19883\n",
      "  Seen 41000 in 2658.62 s\n",
      "  Seen 42000 in 2678.26 s\n",
      "  Seen 43000 in 2697.91 s\n",
      "  Seen 44000 in 2717.75 s\n",
      "  Seen 45000 in 2737.32 s\n",
      "  Seen 46000 in 2757.80 s\n",
      "  Seen 47000 in 2779.47 s\n",
      "  Seen 48000 in 2798.63 s\n",
      "  Seen 49000 in 2818.93 s\n",
      "  Seen 50000 in 2839.52 s\n",
      "  [50000]: mean loss 4.13323\n",
      "  Seen 51000 in 3210.26 s\n",
      "  Seen 52000 in 3232.19 s\n",
      "  Seen 53000 in 3252.64 s\n",
      "  Seen 54000 in 3274.14 s\n",
      "  Seen 55000 in 3293.77 s\n",
      "  Seen 56000 in 3314.84 s\n",
      "  [56522]: mean loss 4.09577\n",
      "SGD complete: 56522 examples in 3683.83 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 9.0099876549519617),\n",
       " (10000, 4.6386767385681313),\n",
       " (20000, 4.3897730782142856),\n",
       " (30000, 4.2912980919415933),\n",
       " (40000, 4.1988311706922934),\n",
       " (50000, 4.1332312755493925),\n",
       " (56522, 4.0957669077505159)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# did not attempt any tuning here...just did straight sgd\n",
    "model = RNNLM(L0, U0 = L0, alpha=0.05, rseed=10, bptt=3)\n",
    "\n",
    "##\n",
    "# Pare down to a smaller dataset, for speed\n",
    "# (optional - recommended to not do this for your final model)\n",
    "ntrain = len(Y_train)\n",
    "X = X_train[:ntrain]\n",
    "Y = Y_train[:ntrain]\n",
    "\n",
    "print 'training set size: %d' % ntrain\n",
    "model.train_sgd(X=X, y=Y, printevery=1000, costevery=10000)\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.14701969121\n"
     ]
    }
   ],
   "source": [
    "## Evaluate cross-entropy loss on the dev set,\n",
    "## then convert to perplexity for your writeup\n",
    "dev_loss = model.compute_mean_loss(X_dev, Y_dev)\n",
    "print dev_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is skewed somewhat by the large number of `UUUNKKK` tokens; if these are 1/6 of the dataset, then that's a sizeable fraction that we're just waving our hands at. Naively, our model gets credit for these that's not really deserved; the formula below roughly removes this contribution from the average loss. Don't worry about how it's derived, but do report both scores - it helps us compare across models with different vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unadjusted: 63.245\n",
      "Adjusted for missing vocab: 98.270\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CELL ##\n",
    "# Report your numbers, after computing dev_loss above.\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Save to .npy files; should only be a few MB total\n",
    "assert(min(model.sparams.L.shape) <= 100) # don't be too big\n",
    "assert(max(model.sparams.L.shape) <= 5000) # don't be too big\n",
    "save(\"rnnlm.L.npy\", model.sparams.L)\n",
    "save(\"rnnlm.U.npy\", model.params.U)\n",
    "save(\"rnnlm.H.npy\", model.params.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g): Generating Data\n",
    "\n",
    "Once you've trained your model to satisfaction, let's use it to generate some sentences!\n",
    "\n",
    "Implement the `generate_sequence` function in `rnnlm.py`, and call it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected 1 of 2000 with p=0.109404\n",
      "selected 254 of 2000 with p=0.000922\n",
      "selected 925 of 2000 with p=0.000204\n",
      "selected 346 of 2000 with p=0.002342\n",
      "selected 863 of 2000 with p=0.000171\n",
      "selected 14 of 2000 with p=0.212176\n",
      "selected 403 of 2000 with p=0.000339\n",
      "selected 9 of 2000 with p=0.310122\n",
      "selected 431 of 2000 with p=0.011205\n",
      "selected 1111 of 2000 with p=0.000049\n",
      "selected 2 of 2000 with p=0.222543\n",
      "selected 5 of 2000 with p=0.999749\n",
      "selected 78 of 2000 with p=0.001395\n",
      "selected 0 of 2000 with p=0.129957\n",
      "selected 74 of 2000 with p=0.000218\n",
      "selected 0 of 2000 with p=0.455104\n",
      "selected 13 of 2000 with p=0.009339\n",
      "selected 226 of 2000 with p=0.002116\n",
      "selected 0 of 2000 with p=0.040343\n",
      "selected 521 of 2000 with p=0.000231\n",
      "selected 255 of 2000 with p=0.000615\n",
      "selected 11 of 2000 with p=0.017121\n",
      "selected 3 of 2000 with p=0.281324\n",
      "selected 6 of 2000 with p=0.017217\n",
      "selected 1108 of 2000 with p=0.000125\n",
      "selected 11 of 2000 with p=0.015899\n",
      "selected 817 of 2000 with p=0.000459\n",
      "selected 645 of 2000 with p=0.000515\n",
      "selected 0 of 2000 with p=0.154260\n",
      "selected 49 of 2000 with p=0.032337\n",
      "selected 89 of 2000 with p=0.000318\n",
      "selected 146 of 2000 with p=0.038624\n",
      "selected 177 of 2000 with p=0.000327\n",
      "selected 418 of 2000 with p=0.000101\n",
      "selected 7 of 2000 with p=0.710621\n",
      "selected 450 of 2000 with p=0.000266\n",
      "selected 9 of 2000 with p=0.066162\n",
      "selected 935 of 2000 with p=0.000048\n",
      "selected 194 of 2000 with p=0.005234\n",
      "selected 9 of 2000 with p=0.050803\n",
      "selected 1857 of 2000 with p=0.000068\n",
      "selected 2 of 2000 with p=0.124117\n",
      "selected 5 of 2000 with p=0.999682\n",
      "selected 2 of 2000 with p=0.094658\n",
      "selected 5 of 2000 with p=0.999818\n",
      "selected 18 of 2000 with p=0.007472\n",
      "selected 1 of 2000 with p=0.020520\n",
      "selected 3 of 2000 with p=0.301813\n",
      "selected 9 of 2000 with p=0.068713\n",
      "selected 606 of 2000 with p=0.003496\n",
      "selected 71 of 2000 with p=0.001663\n",
      "selected 10 of 2000 with p=0.053143\n",
      "selected 3 of 2000 with p=0.266244\n",
      "selected 0 of 2000 with p=0.129490\n",
      "selected 8 of 2000 with p=0.037031\n",
      "selected 1513 of 2000 with p=0.000616\n",
      "selected 41 of 2000 with p=0.000306\n",
      "selected 1817 of 2000 with p=0.000174\n",
      "selected 0 of 2000 with p=0.034118\n",
      "selected 36 of 2000 with p=0.003374\n",
      "selected 304 of 2000 with p=0.003943\n",
      "selected 2 of 2000 with p=0.042580\n",
      "selected 5 of 2000 with p=0.999414\n",
      "selected 52 of 2000 with p=0.002902\n",
      "selected 15 of 2000 with p=0.388817\n",
      "selected 60 of 2000 with p=0.049742\n",
      "selected 28 of 2000 with p=0.687407\n",
      "selected 0 of 2000 with p=0.153536\n",
      "selected 44 of 2000 with p=0.568995\n",
      "selected 14 of 2000 with p=0.613180\n",
      "selected 125 of 2000 with p=0.602181\n",
      "selected 9 of 2000 with p=0.005721\n",
      "selected 635 of 2000 with p=0.000375\n",
      "selected 0 of 2000 with p=0.254361\n",
      "selected 3 of 2000 with p=0.163553\n",
      "selected 19 of 2000 with p=0.127209\n",
      "selected 62 of 2000 with p=0.005027\n",
      "selected 3 of 2000 with p=0.463634\n",
      "selected 1072 of 2000 with p=0.000239\n",
      "selected 870 of 2000 with p=0.000483\n",
      "selected 7 of 2000 with p=0.371157\n",
      "selected 726 of 2000 with p=0.004864\n",
      "selected 54 of 2000 with p=0.046857\n",
      "selected 0 of 2000 with p=0.160178\n",
      "selected 17 of 2000 with p=0.031900\n",
      "selected 3 of 2000 with p=0.090153\n",
      "selected 3 of 2000 with p=0.259523\n",
      "selected 19 of 2000 with p=0.172439\n",
      "selected 2 of 2000 with p=0.687490\n",
      "selected 5 of 2000 with p=0.999184\n",
      "selected 1204 of 2000 with p=0.000054\n",
      "selected 650 of 2000 with p=0.000797\n",
      "selected 7 of 2000 with p=0.602175\n",
      "selected 3 of 2000 with p=0.173753\n",
      "selected 2 of 2000 with p=0.052328\n",
      "selected 5 of 2000 with p=0.999708\n",
      "selected 25 of 2000 with p=0.007513\n",
      "selected 20 of 2000 with p=0.031129\n",
      "selected 59 of 2000 with p=0.022001\n",
      "selected 3 of 2000 with p=0.211335\n",
      "408.423024062\n",
      "<s> the past french ended fees DGDG losses in august term . </s> DG/DG , share , for another , health traders 's UUUNKKK of red 's communications airlines , which last week since likely to systems in staff markets in morris . </s> . </s> is the UUUNKKK in july u.s. and UUUNKKK , a weak have hundreds , an director . </s> about $ DG.DG million , or DGDG cents in account , UUUNKKK said his UUUNKKK media scheduled to # DG.DGDG , '' UUUNKKK UUUNKKK said . </s> discount continued to UUUNKKK . </s> as it had UUUNKKK </s>\n"
     ]
    }
   ],
   "source": [
    "def seq_to_words(seq):\n",
    "    return [num_to_word[s] for s in seq]\n",
    "    \n",
    "seq, J = model.generate_sequence(word_to_num[\"<s>\"], \n",
    "                                 word_to_num[\"</s>\"], \n",
    "                                 maxlen=100)\n",
    "print J\n",
    "# print seq\n",
    "print \" \".join(seq_to_words(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS:** Use the unigram distribution given in the `vocab` table to fill in any `UUUNKKK` tokens in your generated sequences with words that we omitted from the vocabulary. You'll want to use `list(vocab.index)` to get a list of words, and `vocab.freq` to get a list of corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace UUUNKKK with a random unigram,\n",
    "# drawn from vocab that we skipped\n",
    "from nn.math import MultinomialSampler, multinomial_sample\n",
    "def fill_unknowns(words):\n",
    "    #### YOUR CODE HERE ####\n",
    "    ret = words # do nothing; replace this\n",
    "    \n",
    "\n",
    "    #### END YOUR CODE ####\n",
    "    return ret\n",
    "    \n",
    "print \" \".join(fill_unknowns(seq_to_words(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
